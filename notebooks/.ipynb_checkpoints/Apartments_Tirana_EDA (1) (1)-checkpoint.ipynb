{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11373b89-ddc9-4141-b9bd-26305adfc7f2",
   "metadata": {},
   "source": [
    "<!-- ========================= -->\n",
    "<!--  House Price Analysis    -->\n",
    "<!-- ========================= -->\n",
    "\n",
    "<h1 style=\"text-align:center; color:#2c3e50;\">\n",
    "üè† House Price Analysis\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"text-align:center; color:#34495e;\">\n",
    "Exploring Apartment Listings in Tirana\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78980f5-2609-493d-a280-3498003432fb",
   "metadata": {},
   "source": [
    "![Titanic](tirana_prices_notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4ad65-716b-47f0-bdff-57b359ee4d01",
   "metadata": {
    "id": "87e4ad65-716b-47f0-bdff-57b359ee4d01"
   },
   "source": [
    "\n",
    "\n",
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h2 style=\"color:#2c3e50;\">üìå The Problem</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Real estate markets are influenced by many factors such as <b>location</b>, <b>apartment size</b>,\n",
    "<b>floor level</b>, <b>number of rooms</b>, <b>furnishing status</b>, and various amenities.\n",
    "Understanding how these variables affect apartment prices is essential for\n",
    "<b>buyers</b>, <b>sellers</b>, and <b>real estate agents</b>.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "In this notebook, we analyze a <b>JSON dataset</b> containing apartment listings,\n",
    "primarily located in <b>Tirana, Albania</b>. Each listing provides the following information:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üí∂ <b>Price</b> (in EUR)</li>\n",
    "  <li>üìê <b>Surface area</b> (square meters)</li>\n",
    "  <li>üõèÔ∏è <b>Number of rooms</b> (bedrooms, bathrooms, living rooms, balconies)</li>\n",
    "  <li>üè¢ <b>Floor level</b></li>\n",
    "  <li>üõãÔ∏è <b>Furnishing status</b></li>\n",
    "  <li>üìç <b>Location</b> (address and coordinates)</li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h2 style=\"color:#2c3e50;\">üéØ Our Mission</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "The goal of this analysis is to explore, preprocess, and model apartment price data\n",
    "using data science and machine learning techniques.\n",
    "</p>\n",
    "\n",
    "<ol style=\"font-size:15px; line-height:1.8;\">\n",
    "  <li><b>Load and inspect the data</b> from a JSON file</li>\n",
    "  <li><b>Convert JSON ‚Üí Pandas DataFrame</b> for efficient analysis</li>\n",
    "  <li><b>Perform Exploratory Data Analysis (EDA)</b> to uncover patterns and relationships</li>\n",
    "  <li><b>Data Preprocessing</b> ‚Äì Preparing data for machine learning algorithms</li>\n",
    "  <li><b>Train-Test Split</b> ‚Äì Separating training data from evaluation data</li>\n",
    "  <li><b>Baseline Model</b> ‚Äì Building a simple initial predictive model</li>\n",
    "  <li><b>Final Evaluation</b> ‚Äì Comprehensive assessment of the model</li>\n",
    "</ol>\n",
    "\n",
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<p style=\"text-align:center; font-size:14px; color:#7f8c8d;\">\n",
    "üìä This notebook combines data analysis, visualization, and machine learning\n",
    "to gain insights into apartment pricing in Tirana.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f6e27-e392-4b7c-8a79-339904c4fe29",
   "metadata": {
    "id": "5e2f6e27-e392-4b7c-8a79-339904c4fe29"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h2 style=\"color:#2c3e50;\">üì¶ Phase 1: Core Libraries</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We begin by importing the core <b>Python libraries</b> required for\n",
    "<b>data analysis</b>, <b>data manipulation</b>, and <b>visualization</b>.\n",
    "These libraries form the foundation of our workflow and will be used\n",
    "throughout the entire notebook.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "In addition, we configure some basic <b>plotting preferences</b> to ensure\n",
    "that our visualizations are clear, consistent, and easy to interpret.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üìä <b>Pandas</b> ‚Äì data loading, cleaning, and manipulation</li>\n",
    "  <li>üî¢ <b>NumPy</b> ‚Äì numerical operations</li>\n",
    "  <li>üìà <b>Matplotlib / Seaborn</b> ‚Äì data visualization</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406ecbf5-5ca1-4fe5-b3aa-cd4eb00a7cc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "406ecbf5-5ca1-4fe5-b3aa-cd4eb00a7cc6",
    "outputId": "d5f29037-2920-40ee-9019-ebe96bf10b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6c128-8f95-4928-aa63-1814436f76e8",
   "metadata": {
    "id": "3cf6c128-8f95-4928-aa63-1814436f76e8"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h2 style=\"color:#2c3e50;\">üìÇ Phase 2: Data Loading & Exploration</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "In this phase, we load the <code>house_price.json</code> dataset into a\n",
    "<b>Pandas DataFrame</b> to enable efficient data manipulation and analysis.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We begin by examining the <b>shape</b> of the dataset to understand its size,\n",
    "followed by a preview of the <b>raw columns</b> and a few <b>sample rows</b>\n",
    "to gain an initial understanding of the data structure and content.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üì• Load JSON data into a DataFrame</li>\n",
    "  <li>üìê Inspect dataset dimensions (rows √ó columns)</li>\n",
    "  <li>üëÄ Preview column names and example records</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b542dee-fe30-407a-8fce-b5825f2b2795",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6b542dee-fe30-407a-8fce-b5825f2b2795",
    "outputId": "28e6dcaa-4c71-4495-bb28-8c7bc382263b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File house_price.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data from JSON file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhouse_price.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATASET OVERVIEW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of listings (rows): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m JsonReader(\n\u001b[0;32m    792\u001b[0m     path_or_buf,\n\u001b[0;32m    793\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[0;32m    794\u001b[0m     typ\u001b[38;5;241m=\u001b[39mtyp,\n\u001b[0;32m    795\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    796\u001b[0m     convert_axes\u001b[38;5;241m=\u001b[39mconvert_axes,\n\u001b[0;32m    797\u001b[0m     convert_dates\u001b[38;5;241m=\u001b[39mconvert_dates,\n\u001b[0;32m    798\u001b[0m     keep_default_dates\u001b[38;5;241m=\u001b[39mkeep_default_dates,\n\u001b[0;32m    799\u001b[0m     precise_float\u001b[38;5;241m=\u001b[39mprecise_float,\n\u001b[0;32m    800\u001b[0m     date_unit\u001b[38;5;241m=\u001b[39mdate_unit,\n\u001b[0;32m    801\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    802\u001b[0m     lines\u001b[38;5;241m=\u001b[39mlines,\n\u001b[0;32m    803\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    804\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    805\u001b[0m     nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m    806\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    807\u001b[0m     encoding_errors\u001b[38;5;241m=\u001b[39mencoding_errors,\n\u001b[0;32m    808\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    809\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    810\u001b[0m )\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    959\u001b[0m ):\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    968\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File house_price.json does not exist"
     ]
    }
   ],
   "source": [
    "# Load data from JSON file\n",
    "df = pd.read_json(\"house_price.json\")\n",
    "\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(f\"Number of listings (rows): {df.shape[0]}\")\n",
    "print(f\"Number of features (columns): {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn names (raw):\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Preview first 5 rows\n",
    "df.head(330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa027952-3b10-4597-9669-f9d61720f544",
   "metadata": {
    "id": "fa027952-3b10-4597-9669-f9d61720f544"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ade462-e6cb-4035-a97f-c070cddc7f3e",
   "metadata": {
    "id": "95ade462-e6cb-4035-a97f-c070cddc7f3e"
   },
   "source": [
    "<h3 style=\"color:#2c3e50;\">üîç Understanding the Features</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Based on the JSON structure, each row in the dataset represents a\n",
    "<b>single apartment listing</b>. The dataset contains a mix of\n",
    "<b>numerical</b>, <b>categorical</b>, and <b>geographical</b> features\n",
    "that describe the price, size, composition, and location of each property.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color:#34495e;\">üí∂ Price & Size</h4>\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li><code>price_in_euro</code> ‚Äì Final listing price in EUR</li>\n",
    "  <li><code>main_property_price</code> ‚Äì Original price in the listing</li>\n",
    "  <li><code>main_property_property_square</code> ‚Äì Surface area in square meters (m¬≤)</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:#34495e;\">üè† Apartment Composition</h4>\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li><code>main_property_property_composition_bedrooms</code></li>\n",
    "  <li><code>main_property_property_composition_bathrooms</code></li>\n",
    "  <li><code>main_property_property_composition_living_rooms</code></li>\n",
    "  <li><code>main_property_property_composition_balconies</code></li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:#34495e;\">üìç Location Information</h4>\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li><code>main_property_location_city_zone_city_city_name</code> (e.g., <i>Tirana</i>)</li>\n",
    "  <li><code>main_property_location_city_zone_formatted_address</code></li>\n",
    "  <li><code>main_property_location_lat</code> ‚Äì Latitude</li>\n",
    "  <li><code>main_property_location_lng</code> ‚Äì Longitude</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:#34495e;\">üìë Additional Details</h4>\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li><code>main_property_floor</code></li>\n",
    "  <li><code>main_property_furnishing_status</code></li>\n",
    "  <li><code>main_property_property_type</code> (e.g., <i>apartment</i>)</li>\n",
    "  <li><code>main_property_property_status</code> (e.g., <i>for_sale</i>)</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "In the next step, we evaluate <b>data quality</b> by checking for missing values,\n",
    "inconsistencies, and reviewing <b>basic descriptive statistics</b> for these features.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc6d94-3f5d-47b8-bbf2-68584ddb05da",
   "metadata": {
    "id": "04cc6d94-3f5d-47b8-bbf2-68584ddb05da"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìù Column Renaming & Selection</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "To make the dataset easier to work with, we <b>rename columns</b> to clear, descriptive English names and select only the relevant features for analysis.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîÑ <b>Rename columns</b> such as <code>price_in_euro ‚Üí price_eur</code> and <code>main_property_property_square ‚Üí area_sqm</code>.</li>\n",
    "  <li>üóÇÔ∏è <b>Keep only main analysis columns</b> including price, area, number of rooms, furnishing, and location details.</li>\n",
    "  <li>üìä <b>Verify the cleaned DataFrame shape</b> and column order for further analysis.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This ensures that all <b>column names are intuitive</b> and the dataset is <b>ready for exploration and modeling</b>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a90d36-81dd-46fb-9e65-26e588d30eab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58a90d36-81dd-46fb-9e65-26e588d30eab",
    "outputId": "becc33b3-c798-4e6a-c008-019eb671f5a4"
   },
   "outputs": [],
   "source": [
    "# Make a copy and rename columns to clearer English names\n",
    "df_renamed = df.copy()\n",
    "\n",
    "df_renamed = df_renamed.rename(columns={\n",
    "    \"main_property_description_text_content_original_text\": \"description\",\n",
    "    \"main_property_floor\": \"floor\",\n",
    "    \"main_property_furnishing_status\": \"furnishing_status\",\n",
    "    \"main_property_has_carport\": \"has_carport\",\n",
    "    \"main_property_has_elevator\": \"has_elevator\",\n",
    "    \"main_property_has_garage\": \"has_garage\",\n",
    "    \"main_property_has_garden\": \"has_garden\",\n",
    "    \"main_property_has_parking_space\": \"has_parking_space\",\n",
    "    \"main_property_has_terrace\": \"has_terrace\",\n",
    "    \"main_property_location_city_zone_city_city_name\": \"city\",\n",
    "    \"main_property_price_currency\": \"price_currency\",\n",
    "    \"main_property_property_composition_balconies\": \"balconies\",\n",
    "    \"main_property_property_composition_bathrooms\": \"bathrooms\",\n",
    "    \"main_property_property_composition_bedrooms\": \"bedrooms\",\n",
    "    \"main_property_property_composition_kitchens\": \"kitchens\",\n",
    "    \"main_property_property_composition_living_rooms\": \"living_rooms\",\n",
    "    \"main_property_property_status\": \"property_status\",\n",
    "    \"main_property_property_type\": \"property_type\",\n",
    "    \"price_in_euro\": \"price_eur\",\n",
    "    \"main_property_property_square\": \"area_sqm\",\n",
    "    \"main_property_location_lat\": \"lat\",\n",
    "    \"main_property_location_lng\": \"lng\"\n",
    "})\n",
    "\n",
    "print(\"Columns after renaming:\")\n",
    "print(df_renamed.columns.tolist())\n",
    "\n",
    "# Keep and order main analysis columns\n",
    "main_cols = [\n",
    "    \"price_eur\",\n",
    "    \"area_sqm\",\n",
    "    \"floor\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"balconies\",\n",
    "    \"living_rooms\",\n",
    "    \"furnishing_status\",\n",
    "    \"has_elevator\",\n",
    "    \"has_parking_space\",\n",
    "    \"has_garage\",\n",
    "    \"has_terrace\",\n",
    "    \"has_garden\",\n",
    "    \"city\",\n",
    "    \"property_type\",\n",
    "    \"property_status\",\n",
    "    \"price_currency\",\n",
    "    \"description\",\n",
    "    \"lat\",\n",
    "    \"lng\"\n",
    "]\n",
    "\n",
    "# Only keep columns that actually exist\n",
    "main_cols = [c for c in main_cols if c in df_renamed.columns]\n",
    "\n",
    "df_clean = df_renamed[main_cols].copy()\n",
    "\n",
    "print(\"\\nShape of cleaned DataFrame:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfca488-5348-43e0-96f0-903c588efe43",
   "metadata": {
    "id": "bbfca488-5348-43e0-96f0-903c588efe43"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üîç Data Quality Check</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Before performing analysis, we assess the dataset for <b>missing values</b> and review <b>data types</b> to ensure data integrity.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>‚ö†Ô∏è <b>Missing Values:</b> Calculate total missing entries and percentage per column.</li>\n",
    "  <li>üßæ <b>Missing Data Table:</b> Display features with missing values sorted by percentage.</li>\n",
    "  <li>üìå <b>Data Types:</b> Review the types of each column to verify correctness for analysis.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This step ensures that <b>data quality issues are identified</b> and can be addressed before feature engineering or modeling.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340cbed-d47a-46ed-a25b-1e77f72fa94d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d340cbed-d47a-46ed-a25b-1e77f72fa94d",
    "outputId": "a94eea1f-70b4-4d9f-fa8e-ba8478b4cfe7"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDATA QUALITY CHECK\")\n",
    "\n",
    "# Missing values\n",
    "missing_total = df.isna().sum().sum()\n",
    "print(f\"Missing Values (total): {missing_total}\")\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Feature': df.isnull().sum().index,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "print(missing_data)\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData Types (summary):\")\n",
    "print(df_clean.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0e52b-2b39-4ca6-8a58-b7337bf09e3b",
   "metadata": {
    "id": "4ab0e52b-2b39-4ca6-8a58-b7337bf09e3b"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üí∞ Price Outliers Detection</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We identify apartments with <b>price per m¬≤</b> that significantly deviate from the typical range using the <b>interquartile range (IQR)</b> method.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üìä Calculate <b>price per m¬≤</b> for each listing.</li>\n",
    "  <li>üìà Determine the <b>Q1, Q3, and IQR</b> of the price distribution.</li>\n",
    "  <li>‚ö†Ô∏è Flag listings below <b>Q1 - 2√óIQR</b> or above <b>Q3 + 2√óIQR</b> as <b>outliers</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This ensures that extreme prices are identified for further inspection or cleaning before analysis.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a6b76-573c-49ec-ad7e-84fbae06cec3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "001a6b76-573c-49ec-ad7e-84fbae06cec3",
    "outputId": "291d5188-f962-4d19-89ed-b325da6c501c"
   },
   "outputs": [],
   "source": [
    "# Price Outliers Detection\n",
    "df_clean['price_per_sqm'] = df_clean['price_eur'] / df_clean['area_sqm']\n",
    "\n",
    "Q1 = df_clean['price_per_sqm'].quantile(0.25)\n",
    "Q3 = df_clean['price_per_sqm'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 2 * IQR\n",
    "upper = Q3 + 2 * IQR\n",
    "\n",
    "outliers = df_clean[\n",
    "    (df_clean['price_per_sqm'] < lower) |\n",
    "    (df_clean['price_per_sqm'] > upper)\n",
    "]\n",
    "\n",
    "print(f\"Outliers based on price/sqm: {len(outliers)}\")\n",
    "print(f\"Bounds: {lower:.2f} ‚Äì {upper:.2f} ‚Ç¨/sqm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc795bb-d363-435b-946f-ae6e59a87bac",
   "metadata": {
    "id": "1fc795bb-d363-435b-946f-ae6e59a87bac"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìå Scope</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Clean listings by ensuring <b>price</b> and <b>area</b> are accurate.\n",
    "</p>\n",
    "\n",
    "<ol style=\"font-size:15px;\">\n",
    "  <li>Extract <b>price</b> and <b>area</b></li>\n",
    "  <li>Fix <b>missing</b> or <b>incorrect</b> values</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2273e5f-7af7-49b3-a10f-e5c75815dc66",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "<h2 style=\"color:#2c3e50;\">üõ†Ô∏è Phase 3: Exploratory Data Analysis (EDA)</h2>\n",
    "\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\"> Data Cleaning & Auto-Fixing</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This section implements classes to <b>detect inconsistencies</b> in apartment listings and\n",
    "<b>automatically fix or flag issues</b> for further review.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîç <b>PriceExtractor</b>: Extracts <b>total price</b> and <b>price per m¬≤</b> from descriptions using <b>regex</b> and optionally <b>Spacy NLP</b>.</li>\n",
    "  <li>‚úÖ <b>DiscrepancyChecker</b>: Identifies <b>price mismatches</b>, <b>unrealistic areas</b>, <b>price outliers</b>, and <b>duplicate listings</b>.</li>\n",
    "  <li>üõ†Ô∏è <b>AutoFixer</b>: Automatically <b>fixes mismatches</b>, removes <b>unrealistic or duplicate listings</b>, and <b>flags price outliers</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Ensures that the dataset is <b>consistent, accurate, and ready</b> for analysis or visualization.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c27b0-cd2c-435c-8e90-4c02344f92d1",
   "metadata": {
    "id": "e97c27b0-cd2c-435c-8e90-4c02344f92d1"
   },
   "outputs": [],
   "source": [
    "# Install: pip install spacy\n",
    "# Download model: python -m spacy download en_core_web_sm\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    HAS_SPACY = True\n",
    "except:\n",
    "    HAS_SPACY = False\n",
    "    print(\"Spacy not installed. Falling back to regex-based extraction.\")\n",
    "    print(\"Install with: pip install spacy\")\n",
    "    print(\"Then download model: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "\n",
    "class PriceExtractor:\n",
    "    \"\"\"Extract prices from text using multiple strategies\"\"\"\n",
    "\n",
    "    def __init__(self, use_nlp: bool = True):\n",
    "        self.use_nlp = use_nlp and HAS_SPACY\n",
    "\n",
    "    def extract_with_regex(self, text: str) -> Tuple[float, str]:\n",
    "        \"\"\"Fallback regex-based extraction\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            text = str(text).lower()\n",
    "\n",
    "            # Look for price per m¬≤\n",
    "            price_per_m2_patterns = [\n",
    "                r'(\\d+[\\s,]*\\d*)\\s*(?:‚Ç¨|euro)\\s*/\\s*m2',\n",
    "                r'(\\d+[\\s,]*\\d*)\\s*‚Ç¨\\s*/\\s*m¬≤',\n",
    "                r'(\\d+)\\s*(?:‚Ç¨|euro)\\s*(?:per|/)\\s*m(?:etro)?(?:2|¬≤)?'\n",
    "            ]\n",
    "\n",
    "            for pattern in price_per_m2_patterns:\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    try:\n",
    "                        price = float(match.group(1).replace(' ', '').replace(',', '.'))\n",
    "                        return price, 'per_sqm'\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            # Look for prices in EUR/Euro\n",
    "            euro_patterns = [\n",
    "                r'cmimi\\s*[:\\s]*(\\d+[\\s,]*\\d*)\\s*(?:‚Ç¨|euro)',\n",
    "                r'(?:‚Ç¨|euro)\\s*(\\d+[\\s,]*\\d*)',\n",
    "                r'(\\d+[\\s,]*\\d*)\\s*(?:‚Ç¨|euro)'\n",
    "            ]\n",
    "\n",
    "            for pattern in euro_patterns:\n",
    "                matches = re.findall(pattern, text)\n",
    "                if matches:\n",
    "                    try:\n",
    "                        # Take the last (usually the main price)\n",
    "                        price = float(matches[-1].replace(' ', '').replace(',', '.'))\n",
    "                        if price > 100:  # Filter out very small numbers (likely not prices)\n",
    "                            return price, 'total'\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Regex extraction error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def extract_with_nlp(self, text: str) -> Tuple[float, str]:\n",
    "        \"\"\"Extract prices using Spacy NLP\"\"\"\n",
    "        if not self.use_nlp:\n",
    "            return self.extract_with_regex(text)\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            text = str(text).lower()\n",
    "            doc = nlp(text)\n",
    "\n",
    "            prices = []\n",
    "\n",
    "            # Look for patterns: NUMBER + CURRENCY + UNIT\n",
    "            for token in doc:\n",
    "                # Check if token is a number or contains numbers\n",
    "                if token.like_num or any(char.isdigit() for char in token.text):\n",
    "                    try:\n",
    "                        # Get context (next 3 tokens)\n",
    "                        idx = token.i\n",
    "                        context = \" \".join([t.text for t in doc[idx:min(idx+4, len(doc))]])\n",
    "\n",
    "                        # Extract the number\n",
    "                        num_match = re.search(r'(\\d+[\\s,]*\\d*)', token.text)\n",
    "                        if num_match:\n",
    "                            price = float(num_match.group(1).replace(' ', '').replace(',', '.'))\n",
    "\n",
    "                            # Check if followed by currency or unit\n",
    "                            if 'euro' in context or '‚Ç¨' in context:\n",
    "                                if 'm2' in context or 'm¬≤' in context or '/m' in context:\n",
    "                                    return price, 'per_sqm'\n",
    "                                elif price > 100:  # Reasonable price amount\n",
    "                                    prices.append((price, 'total', context))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            # Return the most likely price\n",
    "            if prices:\n",
    "                # Prefer higher prices (main listing price usually higher than rental)\n",
    "                prices.sort(key=lambda x: x[0], reverse=True)\n",
    "                return prices[0][0], prices[0][1]\n",
    "\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NLP extraction error: {e}. Falling back to regex.\")\n",
    "            return self.extract_with_regex(text)\n",
    "\n",
    "    def extract(self, text: str) -> Tuple[float, str]:\n",
    "        \"\"\"Extract price using best available method\"\"\"\n",
    "        if self.use_nlp:\n",
    "            return self.extract_with_nlp(text)\n",
    "        else:\n",
    "            return self.extract_with_regex(text)\n",
    "\n",
    "\n",
    "class DiscrepancyChecker:\n",
    "    \"\"\"Check for inconsistencies in apartment listing data\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, use_nlp: bool = True):\n",
    "        self.df = df.copy()\n",
    "        self.price_extractor = PriceExtractor(use_nlp=use_nlp)\n",
    "\n",
    "    def check_price_per_sqm_stated_vs_calculated(self) -> List[Dict]:\n",
    "        \"\"\"Compare stated price/m¬≤ in description with calculated value\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                stated_price, price_type = self.price_extractor.extract(row['description'])\n",
    "\n",
    "                if price_type == 'per_sqm' and stated_price:\n",
    "                    calculated = row['price_eur'] / row['area_sqm']\n",
    "                    pct_diff = abs(stated_price - calculated) / calculated * 100\n",
    "\n",
    "                    if pct_diff > 15:\n",
    "                        issues.append({\n",
    "                            'id': idx,\n",
    "                            'check': 'price_per_sqm_mismatch',\n",
    "                            'stated': stated_price,\n",
    "                            'calculated': calculated,\n",
    "                            'pct_diff': pct_diff\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def check_area_consistency(self) -> List[Dict]:\n",
    "        \"\"\"Check if areas are realistic\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            for idx, row in self.df.iterrows():\n",
    "                area = row['area_sqm']\n",
    "\n",
    "                # Check overall limits\n",
    "                if pd.isna(area) or area < 9 or area > 1000:\n",
    "                    reason = 'too_small' if area < 18 else 'too_large' if area > 250 else 'missing'\n",
    "                    issues.append({\n",
    "                        'id': idx,\n",
    "                        'check': 'unrealistic_area',\n",
    "                        'area': area,\n",
    "                        'reason': reason\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Area check error: {e}\")\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def check_price_outliers(self, std_multiplier: float = 2.5) -> List[Dict]:\n",
    "        \"\"\"Identify statistical outliers in price per m¬≤\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        try:\n",
    "            # Filter out NaN values\n",
    "            valid_prices = self.df['price_per_sqm'].dropna()\n",
    "\n",
    "            if len(valid_prices) == 0:\n",
    "                return issues\n",
    "\n",
    "            mean = valid_prices.mean()\n",
    "            std = valid_prices.std()\n",
    "\n",
    "            if pd.isna(std) or std == 0:\n",
    "                return issues\n",
    "\n",
    "            threshold = std_multiplier * std\n",
    "\n",
    "            for idx, row in self.df.iterrows():\n",
    "                if pd.isna(row['price_per_sqm']):\n",
    "                    continue\n",
    "\n",
    "                if abs(row['price_per_sqm'] - mean) > threshold:\n",
    "                    z_score = (row['price_per_sqm'] - mean) / std\n",
    "                    issues.append({\n",
    "                        'id': idx,\n",
    "                        'check': 'price_outlier',\n",
    "                        'price_per_sqm': row['price_per_sqm'],\n",
    "                        'z_score': abs(z_score)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Outlier check error: {e}\")\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def check_similar_listings(self, similarity_threshold: float = 0.75) -> List[Dict]:\n",
    "        \"\"\"Find potentially duplicate or very similar listings\"\"\"\n",
    "        issues = []\n",
    "        checked_pairs = set()\n",
    "\n",
    "        try:\n",
    "            for i, row1 in self.df.iterrows():\n",
    "                for j, row2 in self.df.iterrows():\n",
    "                    if i >= j or (i, j) in checked_pairs:\n",
    "                        continue\n",
    "\n",
    "                    checked_pairs.add((i, j))\n",
    "\n",
    "                    try:\n",
    "                        desc1 = str(row1['description']).lower()\n",
    "                        desc2 = str(row2['description']).lower()\n",
    "\n",
    "                        # Extract words safely\n",
    "                        words1 = set(re.findall(r'\\b\\w{4,}\\b', desc1)) if desc1 else set()\n",
    "                        words2 = set(re.findall(r'\\b\\w{4,}\\b', desc2)) if desc2 else set()\n",
    "\n",
    "                        if not words1 or not words2:\n",
    "                            continue\n",
    "\n",
    "                        intersection = len(words1 & words2)\n",
    "                        union = len(words1 | words2)\n",
    "                        similarity = intersection / union if union > 0 else 0\n",
    "\n",
    "                        price_diff = abs(row1['price_eur'] - row2['price_eur'])\n",
    "                        area_diff = abs(row1['area_sqm'] - row2['area_sqm'])\n",
    "\n",
    "                        if similarity > similarity_threshold and price_diff < 5000 and area_diff < 5:\n",
    "                            issues.append({\n",
    "                                'id_1': i,\n",
    "                                'id_2': j,\n",
    "                                'check': 'duplicate',\n",
    "                                'similarity': similarity\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"Duplicate check error: {e}\")\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def run_all_checks(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Execute all consistency checks\"\"\"\n",
    "        results = {\n",
    "            'price_per_sqm_mismatch': self.check_price_per_sqm_stated_vs_calculated(),\n",
    "            'area_consistency': self.check_area_consistency(),\n",
    "            'price_outliers': self.check_price_outliers(),\n",
    "            'duplicates': self.check_similar_listings()\n",
    "        }\n",
    "        return results\n",
    "\n",
    "\n",
    "class AutoFixer:\n",
    "    \"\"\"Automatically fix issues found by DiscrepancyChecker\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.original_df = df.copy()\n",
    "        self.fixes_log = []\n",
    "\n",
    "    def fix_price_per_sqm_mismatch(self, issues: List[Dict]) -> None:\n",
    "        \"\"\"Fix price per m¬≤ mismatches by using the stated price\"\"\"\n",
    "        for issue in issues:\n",
    "            try:\n",
    "                idx = issue['id']\n",
    "                stated = issue['stated']\n",
    "                area = self.df.loc[idx, 'area_sqm']\n",
    "                old_price = self.df.loc[idx, 'price_eur']\n",
    "\n",
    "                new_price = stated * area\n",
    "\n",
    "                self.df.loc[idx, 'price_eur'] = new_price\n",
    "                self.df.loc[idx, 'price_per_sqm'] = stated\n",
    "\n",
    "                self.fixes_log.append({\n",
    "                    'id': idx,\n",
    "                    'issue_type': 'price_per_sqm_mismatch',\n",
    "                    'old_price': old_price,\n",
    "                    'new_price': new_price,\n",
    "                    'action': 'Used stated price/m¬≤ from description'\n",
    "                })\n",
    "\n",
    "                print(f\"ID {idx}: Price corrected ‚Ç¨{old_price:,.0f} ‚Üí ‚Ç¨{new_price:,.0f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error fixing ID {issue['id']}: {e}\")\n",
    "\n",
    "    def fix_unrealistic_areas(self, issues: List[Dict]) -> None:\n",
    "        \"\"\"Remove listings with unrealistic areas\"\"\"\n",
    "        ids_to_remove = [issue['id'] for issue in issues]\n",
    "\n",
    "        try:\n",
    "            for idx in ids_to_remove:\n",
    "                area = self.df.loc[idx, 'area_sqm']\n",
    "\n",
    "                self.fixes_log.append({\n",
    "                    'id': idx,\n",
    "                    'issue_type': 'unrealistic_area',\n",
    "                    'area': area,\n",
    "                    'action': 'Removed listing with unrealistic area'\n",
    "                })\n",
    "\n",
    "                print(f\"ID {idx}: Removed (area: {area} m¬≤ is unrealistic)\")\n",
    "\n",
    "            self.df = self.df.drop(ids_to_remove, errors='ignore')\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing areas: {e}\")\n",
    "\n",
    "    def fix_duplicates(self, issues: List[Dict]) -> None:\n",
    "        \"\"\"Remove duplicate listings\"\"\"\n",
    "        ids_to_remove = []\n",
    "\n",
    "        try:\n",
    "            for issue in issues:\n",
    "                id_1 = issue['id_1']\n",
    "                id_2 = issue['id_2']\n",
    "                similarity = issue['similarity']\n",
    "\n",
    "                remove_id = max(id_1, id_2)\n",
    "                ids_to_remove.append(remove_id)\n",
    "\n",
    "                self.fixes_log.append({\n",
    "                    'id': remove_id,\n",
    "                    'issue_type': 'duplicate',\n",
    "                    'duplicate_of': min(id_1, id_2),\n",
    "                    'similarity': similarity,\n",
    "                    'action': f'Removed duplicate (kept ID {min(id_1, id_2)})'\n",
    "                })\n",
    "\n",
    "                print(f\"ID {remove_id}: Removed (duplicate of ID {min(id_1, id_2)})\")\n",
    "\n",
    "            self.df = self.df.drop(list(set(ids_to_remove)), errors='ignore')\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing duplicates: {e}\")\n",
    "\n",
    "    def fix_price_outliers(self, issues: List[Dict], method: str = 'flag') -> None:\n",
    "        \"\"\"Handle price outliers\"\"\"\n",
    "        try:\n",
    "            if method == 'flag':\n",
    "                for issue in issues:\n",
    "                    self.fixes_log.append({\n",
    "                        'id': issue['id'],\n",
    "                        'issue_type': 'price_outlier',\n",
    "                        'price_per_sqm': issue['price_per_sqm'],\n",
    "                        'z_score': issue['z_score'],\n",
    "                        'action': 'Flagged for manual review'\n",
    "                    })\n",
    "                    print(f\"ID {issue['id']}: Price outlier (z-score: {issue['z_score']:.2f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling outliers: {e}\")\n",
    "\n",
    "    def recalculate_all_metrics(self) -> None:\n",
    "        \"\"\"Ensure all calculations are consistent\"\"\"\n",
    "        try:\n",
    "            self.df['price_per_sqm'] = self.df['price_eur'] / self.df['area_sqm']\n",
    "\n",
    "            self.fixes_log.append({\n",
    "                'id': 'batch',\n",
    "                'issue_type': 'consistency_check',\n",
    "                'rows_recalculated': len(self.df),\n",
    "                'action': 'Recalculated all price_per_sqm values'\n",
    "            })\n",
    "\n",
    "            print(f\"Recalculated price_per_sqm for all {len(self.df)} listings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error recalculating metrics: {e}\")\n",
    "\n",
    "    def auto_fix_all(self, checker_results: Dict) -> None:\n",
    "        \"\"\"Automatically fix all issues\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"AUTO-FIXING ALL DISCREPANCIES\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if checker_results['duplicates']:\n",
    "            print(\"\\n1. REMOVING DUPLICATES\")\n",
    "            print(\"-\"*80)\n",
    "            self.fix_duplicates(checker_results['duplicates'])\n",
    "\n",
    "        if checker_results['area_consistency']:\n",
    "            print(\"\\n2. REMOVING UNREALISTIC AREAS\")\n",
    "            print(\"-\"*80)\n",
    "            self.fix_unrealistic_areas(checker_results['area_consistency'])\n",
    "\n",
    "        if checker_results['price_per_sqm_mismatch']:\n",
    "            print(\"\\n3. FIXING PRICE/M¬≤ MISMATCHES\")\n",
    "            print(\"-\"*80)\n",
    "            self.fix_price_per_sqm_mismatch(checker_results['price_per_sqm_mismatch'])\n",
    "\n",
    "        if checker_results['price_outliers']:\n",
    "            print(\"\\n4. HANDLING PRICE OUTLIERS\")\n",
    "            print(\"-\"*80)\n",
    "            self.fix_price_outliers(checker_results['price_outliers'], method='flag')\n",
    "\n",
    "        print(\"\\n5. RECALCULATING ALL METRICS\")\n",
    "        print(\"-\"*80)\n",
    "        self.recalculate_all_metrics()\n",
    "\n",
    "    def save_results(self, data_file: str = 'apartments_fixed.csv',\n",
    "                     log_file: str = 'fix_log.csv') -> None:\n",
    "        \"\"\"Save fixed data and log\"\"\"\n",
    "        try:\n",
    "            self.df.to_csv(data_file, index=False)\n",
    "            pd.DataFrame(self.fixes_log).to_csv(log_file, index=False)\n",
    "\n",
    "            print(f\"\\nFixed data saved to: {data_file}\")\n",
    "            print(f\"Fix log saved to: {log_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "\n",
    "    def get_fixed_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the fixed dataframe\"\"\"\n",
    "        return self.df\n",
    "\n",
    "    def summary(self) -> None:\n",
    "        \"\"\"Print summary of fixes\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FIX SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Original listings: {len(self.original_df)}\")\n",
    "        print(f\"Fixed listings: {len(self.df)}\")\n",
    "        print(f\"Removed: {len(self.original_df) - len(self.df)}\")\n",
    "        print(f\"Total fixes applied: {len(self.fixes_log)}\")\n",
    "\n",
    "        if self.fixes_log:\n",
    "            fix_types = pd.DataFrame(self.fixes_log)['issue_type'].value_counts()\n",
    "            print(\"\\nFixes by type:\")\n",
    "            for fix_type, count in fix_types.items():\n",
    "                print(f\"  {fix_type}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843176a-ce16-4eba-86a9-198febe4f154",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìê Area Extraction & Updating</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This section defines classes to <b>extract accurate apartment areas</b> from\n",
    "listing descriptions and update the dataframe accordingly.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üè† <b>AreaExtractor</b>: Extracts <b>bruto (gross)</b>, <b>neto (net)</b>, or <b>generic</b> area measurements using regex patterns.</li>\n",
    "  <li>üóÇÔ∏è <b>DataframeAreaUpdater</b>: Updates the dataframe with extracted areas, fills missing values, recalculates <b>price per m¬≤</b>, and logs all changes.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Ensures that <b>area data is consistent and reliable</b> for further analysis and modeling.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d692b-5a1f-489d-8131-fb8e83c5206a",
   "metadata": {
    "id": "b11d692b-5a1f-489d-8131-fb8e83c5206a"
   },
   "outputs": [],
   "source": [
    "class AreaExtractor:\n",
    "    \"\"\"Extract correct area (bruto preferred, neto as fallback) from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Patterns for area extraction - BRUTO (priority 1)\n",
    "        self.bruto_patterns = [\n",
    "            # Standard bruto patterns\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s*(?:bruto|b\\.)',\n",
    "            r'(?:bruto|b\\.)\\s*[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+bruto',\n",
    "            # Siperfaqe reale patterns (highest priority - means \"real area\" = bruto)\n",
    "            r'\\((\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+siperfaqe\\s+reale\\)',  # \"(100m2 siperfaqe reale)\"\n",
    "            r'siperfaqe\\s+reale\\s*[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',  # \"siperfaqe reale: 100m2\"\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+siperfaqe\\s+reale',  # \"100m2 siperfaqe reale\"\n",
    "            # Generic siperfaqe (when only one measurement given)\n",
    "            r'siperfaqe\\s+[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',  # \"siperfaqe: 100m2\"\n",
    "        ]\n",
    "\n",
    "        # Patterns for area extraction - NETO (priority 2)\n",
    "        self.neto_patterns = [\n",
    "            r'(?:siperfaqe\\s+)?neto\\s*[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s*(?:neto|n\\.)',\n",
    "            r'(?:neto|n\\.)\\s*[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+neto',\n",
    "        ]\n",
    "\n",
    "        # Generic area pattern (when neto/bruto not specified)\n",
    "        self.generic_pattern = r'siperfaqe\\s*(?:totale)?\\s*[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?'\n",
    "\n",
    "    def _parse_number(self, text: str) -> float:\n",
    "        \"\"\"Convert text number to float\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        text = text.replace(' ', '').replace(',', '.')\n",
    "        try:\n",
    "            return float(text)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extract_bruto_area(self, text: str) -> float:\n",
    "        \"\"\"Extract bruto (gross) or reale (real) area\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        # First, check for \"siperfaqe reale\" pattern (highest confidence)\n",
    "        reale_patterns = [\n",
    "            r'\\((\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+sip[e√´]rfaqe\\s+reale\\)',\n",
    "            r'\\((\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+siperfaqe\\s+reale\\)',\n",
    "        ]\n",
    "\n",
    "        for pattern in reale_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                value = self._parse_number(match.group(1))\n",
    "                if value and 15 < value < 300:\n",
    "                    return value\n",
    "\n",
    "        # Check for other bruto patterns\n",
    "        for pattern in self.bruto_patterns:\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            if matches:\n",
    "                values = [self._parse_number(m) for m in matches]\n",
    "                values = [v for v in values if v and 15 < v < 300]\n",
    "                if values:\n",
    "                    return max(values)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_neto_area(self, text: str) -> float:\n",
    "        \"\"\"Extract neto (net) area - only if explicitly marked as neto\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        # Only match if explicitly marked with \"neto\" keyword\n",
    "        neto_patterns = [\n",
    "            r'siperfaqe\\s+neto\\s+(\\d+[\\s,]*\\.?\\d*)\\s*m2?',  # \"siperfaqe neto 98.74 m2\"\n",
    "            r'(\\d+[\\s,]*\\.?\\d*)\\s*m2?\\s+(?:siperfaqe\\s+)?neto',  # \"98.74 m2 neto\"\n",
    "            r'neto\\s+[:\\s]*(\\d+[\\s,]*\\.?\\d*)\\s*m2?',  # \"neto: 98.74 m2\"\n",
    "        ]\n",
    "\n",
    "        for pattern in neto_patterns:\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            if matches:\n",
    "                values = [self._parse_number(m) for m in matches]\n",
    "                values = [v for v in values if v and 15 < v < 300]\n",
    "                if values:\n",
    "                    return max(values)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_generic_area(self, text: str) -> float:\n",
    "        \"\"\"Extract generic area when neto/bruto not specified\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "\n",
    "        text = str(text).lower()\n",
    "        matches = re.findall(self.generic_pattern, text)\n",
    "\n",
    "        if matches:\n",
    "            values = [self._parse_number(m) for m in matches]\n",
    "            values = [v for v in values if v and 15 < v < 300]\n",
    "            if values:\n",
    "                # If both bruto and neto mentioned, take larger (bruto)\n",
    "                return max(values)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_best_area(self, text: str) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Extract the best area measurement:\n",
    "        Priority: bruto > neto > generic\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None, None\n",
    "\n",
    "        # Try bruto first\n",
    "        bruto = self.extract_bruto_area(text)\n",
    "        if bruto:\n",
    "            return bruto, 'bruto'\n",
    "\n",
    "        # Fall back to neto\n",
    "        neto = self.extract_neto_area(text)\n",
    "        if neto:\n",
    "            return neto, 'neto'\n",
    "\n",
    "        # Fall back to generic\n",
    "        generic = self.extract_generic_area(text)\n",
    "        if generic:\n",
    "            return generic, 'generic'\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def extract_all_areas(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract all area measurements for verification\"\"\"\n",
    "        return {\n",
    "            'bruto': self.extract_bruto_area(text),\n",
    "            'neto': self.extract_neto_area(text),\n",
    "            'generic': self.extract_generic_area(text)\n",
    "        }\n",
    "\n",
    "    def debug_extract(self, text: str) -> None:\n",
    "        \"\"\"Debug extraction - print all patterns tried\"\"\"\n",
    "        print(f\"\\n--- DEBUG EXTRACTION ---\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        print(\"\\nBRUTO patterns:\")\n",
    "        for i, pattern in enumerate(self.bruto_patterns):\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            if matches:\n",
    "                print(f\"  Pattern {i}: {pattern}\")\n",
    "                print(f\"  Matches: {matches}\")\n",
    "\n",
    "        print(\"\\nNETO patterns:\")\n",
    "        for i, pattern in enumerate(self.neto_patterns):\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            if matches:\n",
    "                print(f\"  Pattern {i}: {pattern}\")\n",
    "                print(f\"  Matches: {matches}\")\n",
    "\n",
    "\n",
    "class DataframeAreaUpdater:\n",
    "    \"\"\"Update dataframe with correct area measurements\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.extractor = AreaExtractor()\n",
    "        self.updates_log = []\n",
    "\n",
    "    def update_areas_from_description(self) -> None:\n",
    "        \"\"\"\n",
    "        Extract areas from descriptions and update the dataframe.\n",
    "        Only updates if description area differs significantly from current area.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"UPDATING AREAS FROM DESCRIPTIONS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            description = row['description']\n",
    "            current_area = row['area_sqm']\n",
    "\n",
    "            extracted_area, area_type = self.extractor.extract_best_area(description)\n",
    "\n",
    "            if extracted_area is None:\n",
    "                continue\n",
    "\n",
    "            # Only update if there's a significant difference (>5% or >3 m¬≤)\n",
    "            if current_area and abs(extracted_area - current_area) > max(3, current_area * 0.05):\n",
    "                old_area = current_area\n",
    "                self.df.loc[idx, 'area_sqm'] = extracted_area\n",
    "\n",
    "                self.updates_log.append({\n",
    "                    'id': idx,\n",
    "                    'old_area': old_area,\n",
    "                    'new_area': extracted_area,\n",
    "                    'source': area_type,\n",
    "                    'reason': f'Updated from description ({area_type})'\n",
    "                })\n",
    "\n",
    "                print(f\"ID {idx}: Area updated {old_area} ‚Üí {extracted_area} m¬≤ ({area_type})\")\n",
    "\n",
    "            elif current_area is None and extracted_area:\n",
    "                self.df.loc[idx, 'area_sqm'] = extracted_area\n",
    "\n",
    "                self.updates_log.append({\n",
    "                    'id': idx,\n",
    "                    'old_area': None,\n",
    "                    'new_area': extracted_area,\n",
    "                    'source': area_type,\n",
    "                    'reason': f'Filled missing area from description ({area_type})'\n",
    "                })\n",
    "\n",
    "                print(f\"ID {idx}: Area filled with {extracted_area} m¬≤ ({area_type})\")\n",
    "\n",
    "    def recalculate_price_per_sqm(self) -> None:\n",
    "        \"\"\"Recalculate price/m¬≤ after area updates\"\"\"\n",
    "        self.df['price_per_sqm'] = self.df['price_eur'] / self.df['area_sqm']\n",
    "        print(\"\\nRecalculated all price_per_sqm values after area updates\")\n",
    "\n",
    "    def verify_areas(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a verification report showing what was extracted from descriptions\n",
    "        \"\"\"\n",
    "        verification = []\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            all_areas = self.extractor.extract_all_areas(row['description'])\n",
    "            verification.append({\n",
    "                'id': idx,\n",
    "                'current_area': row['area_sqm'],\n",
    "                'bruto_from_desc': all_areas['bruto'],\n",
    "                'neto_from_desc': all_areas['neto'],\n",
    "                'generic_from_desc': all_areas['generic'],\n",
    "                'mismatch': (row['area_sqm'] != all_areas['bruto'] and\n",
    "                           row['area_sqm'] != all_areas['neto'] if all_areas['bruto'] or all_areas['neto'] else False)\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(verification)\n",
    "\n",
    "    def get_updated_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the updated dataframe\"\"\"\n",
    "        return self.df\n",
    "\n",
    "    def save_updates(self, csv_file: str = 'area_updates.csv') -> None:\n",
    "        \"\"\"Save log of area updates\"\"\"\n",
    "        if self.updates_log:\n",
    "            pd.DataFrame(self.updates_log).to_csv(csv_file, index=False)\n",
    "            print(f\"\\n Area updates log saved to: {csv_file}\")\n",
    "        else:\n",
    "            print(\"\\nNo area updates were made\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad7e9d-6a21-434c-81b5-93a47e9946d7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üõ†Ô∏è Selective Outlier Update Workflow</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This workflow <b>cleans only the outlier rows</b> in the dataset and then updates the main dataframe.\n",
    "It ensures that corrections are applied selectively, while preserving the integrity of valid data.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Stage 1: Clean Outlier Subset</b> ‚Äì Identify and fix <b>price mismatches</b>, <b>unrealistic areas</b>, <b>price outliers</b>, and <b>duplicates</b> using <code>DiscrepancyChecker</code> and <code>AutoFixer</code>.</li>\n",
    "  <li>üîπ <b>Stage 2: Merge Back</b> ‚Äì Update the main dataframe with the cleaned outlier rows and remove duplicates or unrealistic entries.</li>\n",
    "  <li>üîπ <b>Stage 3: Re-check</b> ‚Äì Recalculate <b>price per m¬≤</b> and flag any remaining outliers in the full dataset.</li>\n",
    "  <li>üîπ <b>Stage 4: Final Analysis DataFrame</b> ‚Äì Drop flagged outliers and create a clean dataframe ready for <b>EDA</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This process maintains <b>data consistency</b>, ensures accurate <b>price per m¬≤ calculations</b>, and provides a clear log of changes for traceability.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674cc0c-e0a2-48a6-aa9a-52faaf59119b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3674cc0c-e0a2-48a6-aa9a-52faaf59119b",
    "outputId": "42080131-bd42-4bcc-934e-81145cd670a5"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SELECTIVE UPDATE WORKFLOW\n",
    "# Fix only outlier rows, then update those rows in main dataframe\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 1: CLEAN OUTLIERS DATAFRAME (FAST)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STAGE 1: CLEANING OUTLIER SUBSET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Outlier subset size: {len(outliers)}\\n\")\n",
    "\n",
    "# Apply DiscrepancyChecker to outliers\n",
    "checker = DiscrepancyChecker(outliers, use_nlp=HAS_SPACY)\n",
    "results = checker.run_all_checks()\n",
    "\n",
    "print(f\"Issues found:\")\n",
    "print(f\"  - Price per m¬≤ mismatches: {len(results['price_per_sqm_mismatch'])}\")\n",
    "print(f\"  - Unrealistic areas: {len(results['area_consistency'])}\")\n",
    "print(f\"  - Price outliers: {len(results['price_outliers'])}\")\n",
    "print(f\"  - Duplicates: {len(results['duplicates'])}\\n\")\n",
    "\n",
    "# Auto-fix outliers\n",
    "fixer = AutoFixer(outliers)\n",
    "fixer.auto_fix_all(results)\n",
    "fixed_outliers = fixer.get_fixed_dataframe()\n",
    "fixer.summary()\n",
    "\n",
    "# Apply AreaExtractor to fixed outliers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING AREAS FROM OUTLIER DESCRIPTIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "area_updater = DataframeAreaUpdater(fixed_outliers)\n",
    "area_updater.update_areas_from_description()\n",
    "area_updater.recalculate_price_per_sqm()\n",
    "\n",
    "final_outliers = area_updater.get_updated_dataframe()\n",
    "\n",
    "print(f\"\\n Outliers cleaned: {len(final_outliers)} listings remain\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 2: MERGE CLEANED OUTLIERS BACK INTO MAIN DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: UPDATING MAIN DATAFRAME WITH CLEANED OUTLIER ROWS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Keep track of which rows were cleaned\n",
    "cleaned_indices = final_outliers.index.tolist()\n",
    "num_cleaned = len(cleaned_indices)\n",
    "\n",
    "print(f\"Rows to be updated in main dataframe: {num_cleaned}\")\n",
    "\n",
    "# Update the main dataframe with cleaned outlier data\n",
    "# Use .loc to update only the rows that were in the outliers subset\n",
    "for col in final_outliers.columns:\n",
    "    df_clean.loc[cleaned_indices, col] = final_outliers[col]\n",
    "\n",
    "print(f\"Updated {num_cleaned} rows in main dataframe\")\n",
    "\n",
    "# Also update rows that were removed as duplicates/unrealistic\n",
    "# (they're not in final_outliers anymore because AutoFixer dropped them)\n",
    "rows_removed_from_outliers = set(outliers.index) - set(final_outliers.index)\n",
    "print(f\"\\nRows removed during outlier cleaning: {len(rows_removed_from_outliers)}\")\n",
    "if len(rows_removed_from_outliers) > 0:\n",
    "    print(f\"  Removing these from main dataframe...\")\n",
    "    df_clean = df_clean.drop(list(rows_removed_from_outliers), errors='ignore')\n",
    "    print(f\"  Removed {len(rows_removed_from_outliers)} rows\")\n",
    "\n",
    "print(f\"\\nMain dataframe after update: {len(df_clean)} listings\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 3: RE-CHECK ENTIRE MAIN DATAFRAME FOR OUTLIERS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 3: RE-CHECKING MAIN DATAFRAME FOR OUTLIERS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Recalculate price_per_sqm for entire main dataframe\n",
    "df_clean['price_per_sqm'] = df_clean['price_eur'] / df_clean['area_sqm']\n",
    "\n",
    "Q1 = df_clean['price_per_sqm'].quantile(0.25)\n",
    "Q3 = df_clean['price_per_sqm'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 2 * IQR\n",
    "upper = Q3 + 2 * IQR\n",
    "\n",
    "df_clean['is_price_outlier'] = (\n",
    "    (df_clean['price_per_sqm'] < lower) |\n",
    "    (df_clean['price_per_sqm'] > upper)\n",
    ").astype(int)\n",
    "\n",
    "num_outliers_new = df_clean['is_price_outlier'].sum()\n",
    "\n",
    "print(f\"Price outlier bounds: {lower:.2f} ‚Äì {upper:.2f} ‚Ç¨/sqm\")\n",
    "print(f\"Outliers in main dataframe: {num_outliers_new}\")\n",
    "\n",
    "if num_outliers_new > 0:\n",
    "    print(f\"\\nRemaining outliers:\")\n",
    "    remaining_outliers = df_clean[df_clean['is_price_outlier'] == 1][\n",
    "        ['price_eur', 'area_sqm', 'price_per_sqm', 'bedrooms', 'bathrooms', 'city']\n",
    "    ].sort_values('price_per_sqm', ascending=False)\n",
    "    print(remaining_outliers.head(10))\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 4: DROP OUTLIERS & CREATE FINAL ANALYSIS DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 4: CREATING FINAL ANALYSIS DATAFRAME\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_analysis = df_clean[df_clean['is_price_outlier'] == 0].copy()\n",
    "\n",
    "print(f\"Data flow summary:\")\n",
    "print(f\"  Original size:           {len(df_clean) + num_outliers_new:,} listings\")\n",
    "print(f\"  After selective fixes:   {len(df_clean):,} listings\")\n",
    "print(f\"  Outliers identified:     {num_outliers_new:,} listings\")\n",
    "print(f\"  Final clean dataset:     {len(df_analysis):,} listings\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATA SUMMARY - READY FOR EDA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary_stats = df_analysis[['price_eur', 'area_sqm', 'price_per_sqm']].describe()\n",
    "print(summary_stats)\n",
    "\n",
    "print(\"\\nMissing values in final dataframe:\")\n",
    "missing = df_analysis.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"df_analysis is ready for EDA!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa357fd1-5e03-4549-bf44-d66482ae682d",
   "metadata": {
    "id": "aa357fd1-5e03-4549-bf44-d66482ae682d"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìä Outlier Removal & Price/m¬≤ Comparison</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This analysis visualizes the impact of outlier removal on the <b>price per m¬≤</b> distribution. \n",
    "It includes histograms, boxplots, and log-scale overlays, along with a detailed comparison table.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Histograms:</b> Show before vs. after outlier removal (95th percentile view) with mean & median lines.</li>\n",
    "  <li>üîπ <b>Boxplots:</b> Compare distribution spread before and after cleaning.</li>\n",
    "  <li>üîπ <b>Log-scale overlay:</b> Shows full range of price/m¬≤ values highlighting removed extreme values.</li>\n",
    "  <li>üîπ <b>Summary Table:</b> Key metrics (mean, median, std, min/max, IQR, percentiles) before and after outlier removal.</li>\n",
    "  <li>üîπ <b>Percent Changes:</b> Shows reduction in listings, change in mean/median/std, and max price improvement.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This workflow helps ensure <b>clean, consistent price data</b> for reliable exploratory data analysis (EDA) and modeling.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4443c-65fc-495c-9ed3-be058237d22d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d1c4443c-65fc-495c-9ed3-be058237d22d",
    "outputId": "fac550fd-37ca-4b58-f1b7-30d409b4cb30"
   },
   "outputs": [],
   "source": [
    "# Create figure with 3 comparison views\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# ============================================================================\n",
    "# Row 1: Histograms with same scale (limited x-axis)\n",
    "# ============================================================================\n",
    "\n",
    "# BEFORE: Limited to 95th percentile for visibility\n",
    "p95_before = df_clean['price_per_sqm'].quantile(0.95)\n",
    "\n",
    "axes[0, 0].hist(df_clean['price_per_sqm'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].axvline(df_clean['price_per_sqm'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ‚Ç¨{df_clean[\"price_per_sqm\"].mean():.0f}')\n",
    "axes[0, 0].axvline(df_clean['price_per_sqm'].median(), color='orange', linestyle='--', linewidth=2, label=f'Median: ‚Ç¨{df_clean[\"price_per_sqm\"].median():.0f}')\n",
    "axes[0, 0].set_xlim(0, p95_before * 1.2)\n",
    "axes[0, 0].set_title(\"BEFORE Outlier Removal (95th percentile view)\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel(\"Price per m¬≤ (‚Ç¨)\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistics\n",
    "stats_before = f\"Count: {len(df_clean):,}\\nMean: ‚Ç¨{df_clean['price_per_sqm'].mean():.0f}\\nMedian: ‚Ç¨{df_clean['price_per_sqm'].median():.0f}\\nStd: ‚Ç¨{df_clean['price_per_sqm'].std():.0f}\\nMax: ‚Ç¨{df_clean['price_per_sqm'].max():.0f}\"\n",
    "axes[0, 0].text(0.98, 0.97, stats_before, transform=axes[0, 0].transAxes,\n",
    "                fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "# AFTER: Same scale for comparison\n",
    "axes[0, 1].hist(df_analysis['price_per_sqm'], bins=50, alpha=0.7, color='seagreen', edgecolor='black')\n",
    "axes[0, 1].axvline(df_analysis['price_per_sqm'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ‚Ç¨{df_analysis[\"price_per_sqm\"].mean():.0f}')\n",
    "axes[0, 1].axvline(df_analysis['price_per_sqm'].median(), color='orange', linestyle='--', linewidth=2, label=f'Median: ‚Ç¨{df_analysis[\"price_per_sqm\"].median():.0f}')\n",
    "axes[0, 1].set_xlim(0, p95_before * 1.2)\n",
    "axes[0, 1].set_title(\"AFTER Outlier Removal (Same scale)\", fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel(\"Price per m¬≤ (‚Ç¨)\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistics\n",
    "stats_after = f\"Count: {len(df_analysis):,}\\nMean: ‚Ç¨{df_analysis['price_per_sqm'].mean():.0f}\\nMedian: ‚Ç¨{df_analysis['price_per_sqm'].median():.0f}\\nStd: ‚Ç¨{df_analysis['price_per_sqm'].std():.0f}\\nMax: ‚Ç¨{df_analysis['price_per_sqm'].max():.0f}\"\n",
    "axes[0, 1].text(0.98, 0.97, stats_after, transform=axes[0, 1].transAxes,\n",
    "                fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# ============================================================================\n",
    "# Row 2: Box plots for distribution comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Prepare data for box plot\n",
    "box_data = [df_clean['price_per_sqm'], df_analysis['price_per_sqm']]\n",
    "bp = axes[1, 0].boxplot(box_data, labels=['Before', 'After'], patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['steelblue', 'seagreen']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[1, 0].set_ylabel(\"Price per m¬≤ (‚Ç¨)\")\n",
    "axes[1, 0].set_title(\"Distribution Comparison (Box Plot)\", fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# Row 2 Right: Log scale histogram (shows full range)\n",
    "# ============================================================================\n",
    "\n",
    "axes[1, 1].hist(df_clean['price_per_sqm'], bins=50, alpha=0.5, color='steelblue', label='Before', edgecolor='black')\n",
    "axes[1, 1].hist(df_analysis['price_per_sqm'], bins=50, alpha=0.5, color='seagreen', label='After', edgecolor='black')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].set_title(\"Both Distributions Overlaid (Log Scale)\", fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel(\"Price per m¬≤ (‚Ç¨)\")\n",
    "axes[1, 1].set_ylabel(\"Frequency (log scale)\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED COMPARISON TABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER REMOVAL IMPACT ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison = {\n",
    "    'Metric': [\n",
    "        'Total Listings',\n",
    "        'Mean Price/m¬≤',\n",
    "        'Median Price/m¬≤',\n",
    "        'Std Dev',\n",
    "        'Min Price/m¬≤',\n",
    "        'Max Price/m¬≤',\n",
    "        'Range',\n",
    "        '25th Percentile',\n",
    "        '75th Percentile',\n",
    "        'IQR'\n",
    "    ],\n",
    "    'Before': [\n",
    "        f\"{len(df_clean):,}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].mean():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].median():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].std():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].min():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].max():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].max() - df_clean['price_per_sqm'].min():.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].quantile(0.25):.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].quantile(0.75):.2f}\",\n",
    "        f\"‚Ç¨{df_clean['price_per_sqm'].quantile(0.75) - df_clean['price_per_sqm'].quantile(0.25):.2f}\"\n",
    "    ],\n",
    "    'After': [\n",
    "        f\"{len(df_analysis):,}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].mean():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].median():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].std():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].min():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].max():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].max() - df_analysis['price_per_sqm'].min():.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].quantile(0.25):.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].quantile(0.75):.2f}\",\n",
    "        f\"‚Ç¨{df_analysis['price_per_sqm'].quantile(0.75) - df_analysis['price_per_sqm'].quantile(0.25):.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate percent changes\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CHANGES:\")\n",
    "print(\"-\"*80)\n",
    "pct_removed = ((len(df_clean) - len(df_analysis)) / len(df_clean)) * 100\n",
    "mean_change = ((df_analysis['price_per_sqm'].mean() - df_clean['price_per_sqm'].mean()) /\n",
    "               df_clean['price_per_sqm'].mean()) * 100\n",
    "std_change = ((df_analysis['price_per_sqm'].std() - df_clean['price_per_sqm'].std()) /\n",
    "              df_clean['price_per_sqm'].std()) * 100\n",
    "median_change = ((df_analysis['price_per_sqm'].median() - df_clean['price_per_sqm'].median()) /\n",
    "                 df_clean['price_per_sqm'].median()) * 100\n",
    "\n",
    "print(f\"Listings removed: {len(df_clean) - len(df_analysis):,} ({pct_removed:.1f}%)\")\n",
    "print(f\"Mean price/m¬≤ change: {mean_change:+.2f}%\")\n",
    "print(f\"Median price/m¬≤ change: {median_change:+.2f}%\")\n",
    "print(f\"Std Dev change: {std_change:+.2f}%\")\n",
    "print(f\"Max price/m¬≤ reduced by: ‚Ç¨{df_clean['price_per_sqm'].max() - df_analysis['price_per_sqm'].max():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bed8ef-0888-4505-a7f1-e2b2f6426b4a",
   "metadata": {
    "id": "43bed8ef-0888-4505-a7f1-e2b2f6426b4a"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "In this phase, we perform <b>feature engineering</b> to enhance the dataset and\n",
    "prepare it for analysis or modeling.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We start by computing a <b>statistical summary</b> of all numeric columns,\n",
    "including count, mean, standard deviation, min, quartiles, and max, to quickly\n",
    "understand <b>price ranges</b> and <b>apartment sizes</b>.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üìä Generate statistical summaries for numeric features</li>\n",
    "  <li>üí° Identify trends and variability in prices and areas</li>\n",
    "  <li>üîç Highlight potential insights for further analysis</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c132d-566c-47f2-9154-6993e3700b6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "id": "870c132d-566c-47f2-9154-6993e3700b6d",
    "outputId": "1a9ebfcc-6580-42c8-c6cc-93d9aa2fb9be"
   },
   "outputs": [],
   "source": [
    "numeric_cols = df_analysis.select_dtypes(include=[np.number]).columns\n",
    "print(\"NUMERIC COLUMNS:\")\n",
    "print(list(numeric_cols))\n",
    "\n",
    "print(\"\\nSTATISTICAL SUMMARY (numeric features):\")\n",
    "df_analysis[numeric_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560c6bd-ab9d-4b8f-b39b-f2dd14b67f9d",
   "metadata": {
    "id": "a560c6bd-ab9d-4b8f-b39b-f2dd14b67f9d"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üßπ 3.2 Cleaning and Feature Engineering</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "For accurate price analysis, we ensure <b>valid price and size information</b> in the dataset.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üóëÔ∏è Drop rows with missing <code>price_in_euro</code> or <code>main_property_property_square</code></li>\n",
    "  <li>‚ùå Remove rows where apartment size is zero or negative</li>\n",
    "  <li>‚ûï Add new features: <code>distance_from_center</code> and <code>total_rooms</code></li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "These steps ensure the dataset is clean and enriched for further analysis.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ec244-8258-469f-9bba-521d79a54d79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e76ec244-8258-469f-9bba-521d79a54d79",
    "outputId": "aae5e876-4544-4e2f-cb45-2a51b33bfecc"
   },
   "outputs": [],
   "source": [
    "print(\"\\nShape of cleaned DataFrame:\", df_analysis.shape)\n",
    "\n",
    "# ======================================================================\n",
    "# FIX NEGATIVE / NON-SENSICAL NUMERIC VALUES\n",
    "# ======================================================================\n",
    "# Columns that should NOT be negative (counts, floors, area, price)\n",
    "non_negative_cols = [\n",
    "    \"price_eur\",\n",
    "    \"area_sqm\",\n",
    "    \"floor\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"balconies\",\n",
    "    \"living_rooms\"\n",
    "]\n",
    "\n",
    "for col in non_negative_cols:\n",
    "    if col in df_analysis.columns:\n",
    "        # Turn negative values (< 0) into NaN so they are treated as missing\n",
    "        df_analysis.loc[df_analysis[col] < 0, col] = np.nan\n",
    "\n",
    "\n",
    "print(\"\\nAFTER basic filters (apartment / for_sale):\", df_analysis.shape)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADD DISTANCE FROM CITY CENTER FEATURE\n",
    "# ============================================================================\n",
    "# Tirana city center coordinates\n",
    "TIRANA_CENTER_LAT = 41.3275\n",
    "TIRANA_CENTER_LNG = 19.8187\n",
    "\n",
    "print(f\"Tirana center coordinates: ({TIRANA_CENTER_LAT}, {TIRANA_CENTER_LNG})\\n\")\n",
    "\n",
    "# Function to calculate distance between two coordinates (Haversine formula)\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    Returns distance in kilometers\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "    # Radius of earth in kilometers\n",
    "    R = 6371\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Calculate distance from center for all listings\n",
    "df_analysis['distance_from_center'] = df_analysis.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        TIRANA_CENTER_LAT,\n",
    "        TIRANA_CENTER_LNG,\n",
    "        row['lat'],\n",
    "        row['lng']\n",
    "    ) if pd.notna(row['lat']) and pd.notna(row['lng']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DISTANCE_FROM_CENTER STATISTICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(df_analysis['distance_from_center'].describe())\n",
    "\n",
    "print(f\"\\nDistance range: {df_analysis['distance_from_center'].min():.2f} km to {df_analysis['distance_from_center'].max():.2f} km\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# IMPUTE MISSING VALUES FOR ALL REMAINING COLUMNS ‚Üí df_model\n",
    "# ======================================================================\n",
    "df_model = df_analysis.copy()\n",
    "\n",
    "# Numeric columns ‚Üí fill NaN with median\n",
    "num_cols = df_model.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "for col in num_cols:\n",
    "    median_val = df_model[col].median()\n",
    "    df_model[col] = df_model[col].fillna(median_val)\n",
    "\n",
    "# Categorical / object columns ‚Üí fill NaN with most frequent value (mode)\n",
    "cat_cols = df_model.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    mode_val = df_model[col].mode(dropna=True)\n",
    "    if len(mode_val) > 0:\n",
    "        df_model[col] = df_model[col].fillna(mode_val[0])\n",
    "\n",
    "print(\"\\nAFTER IMPUTATION (df_model ready for EDA / modeling)\")\n",
    "print(\"Shape:\", df_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041a145-a7ed-43f9-bbae-0d65b7ae2bcd",
   "metadata": {
    "id": "f041a145-a7ed-43f9-bbae-0d65b7ae2bcd"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìä Apartment Market Feature Distributions</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This section visualizes the distributions of key numerical features in the apartment dataset. \n",
    "Extreme outliers are capped at the 99th percentile to improve clarity.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Total Price (EUR)</b> ‚Äì Shows the distribution of apartment prices with a 99th percentile cap to reduce the influence of extreme values.</li>\n",
    "  <li>üîπ <b>Area (sqm)</b> ‚Äì Visualizes the distribution of apartment sizes, capped at the 99th percentile.</li>\n",
    "  <li>üîπ <b>Price per sqm (EUR/sqm)</b> ‚Äì Highlights the variation in price per square meter across listings.</li>\n",
    "  <li>üîπ <b>Distance from Center (Km)</b> ‚Äì Displays the density of apartments‚Äô distance from the city center, limited to the 99th percentile and 0‚Äì20 km range for readability.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "These histograms help identify typical ranges, central tendencies, and potential anomalies in the dataset, providing a foundation for further exploratory data analysis (EDA) and feature engineering.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a2ce9-0d05-415f-a534-83d300c16220",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "123a2ce9-0d05-415f-a534-83d300c16220",
    "outputId": "86489c63-10dc-48fd-a6f1-64ed40ed5eba"
   },
   "outputs": [],
   "source": [
    "# Histogram of total price in EUR (with upper cap to reduce influence of extreme outliers)\n",
    "plt.figure()\n",
    "sns.histplot(data=df_model[df_model[\"price_eur\"] < df_model[\"price_eur\"].quantile(0.99)],\n",
    "             x=\"price_eur\", bins=50, kde=True)\n",
    "plt.title(\"Distribution of Apartment Prices (EUR) ‚Äì 99th Percentile Cap\")\n",
    "plt.xlabel(\"Price in EUR\")\n",
    "plt.ylabel(\"Count of listings\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of area in square meters\n",
    "plt.figure()\n",
    "sns.histplot(data=df_model[df_model[\"area_sqm\"] < df_model[\"area_sqm\"].quantile(0.99)],\n",
    "             x=\"area_sqm\", bins=50, kde=True, color=\"orange\")\n",
    "plt.title(\"Distribution of Apartment Area (sqm) ‚Äì 99th Percentile Cap\")\n",
    "plt.xlabel(\"Area (sqm)\")\n",
    "plt.ylabel(\"Count of listings\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of price per square meter\n",
    "plt.figure()\n",
    "sns.histplot(data=df_model[df_model[\"price_per_sqm\"] < df_model[\"price_per_sqm\"].quantile(0.99)],\n",
    "             x=\"price_per_sqm\", bins=50, kde=True, color=\"green\")\n",
    "plt.title(\"Distribution of Price per Square Meter (EUR/sqm)\")\n",
    "plt.xlabel(\"EUR per sqm\")\n",
    "plt.ylabel(\"Count of listings\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of distance from the center\n",
    "plt.figure()\n",
    "sns.histplot(\n",
    "    data=df_model[df_model[\"distance_from_center\"] < df_model[\"distance_from_center\"].quantile(0.99)],\n",
    "    x=\"distance_from_center\",\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    stat=\"density\",\n",
    "    color=\"green\"\n",
    ")\n",
    "\n",
    "plt.title(\"Density of Distance from Center\")\n",
    "plt.xlabel(\"Km\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(0, 20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea02e0-ba80-4186-910f-5553fcd741bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "f8ea02e0-ba80-4186-910f-5553fcd741bb",
    "outputId": "59d8a031-a25e-496a-a267-2386ec60d303"
   },
   "outputs": [],
   "source": [
    "# Create area groups for simpler visualization\n",
    "df_model[\"area_group\"] = pd.cut(\n",
    "    df_model[\"area_sqm\"],\n",
    "    bins=[0, 40, 60, 80, 100, 150, 300],\n",
    "    labels=[\"0-40\", \"40-60\", \"60-80\", \"80-100\", \"100-150\", \"150+\"]\n",
    ")\n",
    "\n",
    "# Calculate mean price for each group\n",
    "area_mean = df_model.groupby(\"area_group\")[\"price_eur\"].mean().reset_index()\n",
    "\n",
    "# Line chart\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(area_mean[\"area_group\"], area_mean[\"price_eur\"], marker=\"o\", linewidth=2)\n",
    "plt.title(\"Average Price (EUR) by Apartment Size\", fontsize=14)\n",
    "plt.xlabel(\"Area Group (sqm)\", fontsize=12)\n",
    "plt.ylabel(\"Average Price (EUR)\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98bee2-9dc2-4400-9e2f-4987541b6e57",
   "metadata": {
    "id": "5f98bee2-9dc2-4400-9e2f-4987541b6e57"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This chart shows how the <b>average apartment price</b> changes with <b>size</b>, highlighting the trend of increasing price as area grows.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779f0d6-af22-4164-8051-743231f62c38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "2779f0d6-af22-4164-8051-743231f62c38",
    "outputId": "8bb73f27-e0de-424d-d2cb-62e513635d48"
   },
   "outputs": [],
   "source": [
    "# If floor_group is not created yet:\n",
    "df_model[\"floor_group\"] = pd.cut(\n",
    "    df_model[\"floor\"],\n",
    "    bins=[-2, 0, 3, 6, 10, 40],\n",
    "    labels=[\"Basement/0\", \"1-3\", \"4-6\", \"7-10\", \">10\"]\n",
    ")\n",
    "\n",
    "# Calculate mean ‚Ç¨/m¬≤ per floor group\n",
    "floor_mean = df_model.groupby(\"floor_group\")[\"price_per_sqm\"].mean().reset_index()\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(floor_mean[\"floor_group\"], floor_mean[\"price_per_sqm\"])\n",
    "plt.title(\"Average Price per sqm by Floor Group\", fontsize=14)\n",
    "plt.xlabel(\"Floor Group\", fontsize=12)\n",
    "plt.ylabel(\"EUR per sqm\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdab36-47b8-4e10-afde-b67bccef8b5a",
   "metadata": {
    "id": "a5cdab36-47b8-4e10-afde-b67bccef8b5a"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This chart shows the <b>average price per square meter</b> for each <b>floor group</b>, illustrating how price per sqm varies depending on the apartment‚Äôs floor level.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7cbd7-1aec-4e6e-b407-d7035e67517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Price vs Area (cleaned data)\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6b270-08e3-47e5-af9a-114c5fb83296",
   "metadata": {
    "id": "0cb6b270-08e3-47e5-af9a-114c5fb83296"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìä Correlation Matrix for Numeric Features</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We examine the <b>linear correlations</b> between the main numeric variables, including <code>price_per_sqm</code>. \n",
    "This helps identify which features are most strongly associated with overall price and price per square meter, providing insights for further analysis and modeling.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd425f4-9183-49aa-b76f-8f20f2282dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fcd425f4-9183-49aa-b76f-8f20f2282dd1",
    "outputId": "5cd62a4b-a860-4898-96fd-8b810782164e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559eaaf7-41b5-44fd-86f6-df4c6b04a893",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üè† Total Rooms & Correlation Matrix</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We create a new feature <b>total_rooms</b> by summing all room types (bedrooms, bathrooms, living rooms, kitchens, balconies). \n",
    "The updated correlation matrix shows how <b>total_rooms</b> and other numeric features relate to apartment price and price per square meter.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9595c-f0c3-44d7-b532-1aa368cd428c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "22e9595c-f0c3-44d7-b532-1aa368cd428c",
    "outputId": "477d1e0d-b5ba-4150-c378-7577885e5d67"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADD TOTAL ROOMS FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADDING TOTAL_ROOMS FEATURE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check which room columns exist\n",
    "room_columns = ['bedrooms', 'bathrooms', 'living_rooms', 'kitchens', 'balconies']\n",
    "available_cols = [col for col in room_columns if col in df_model.columns]\n",
    "\n",
    "print(f\"Available room columns: {available_cols}\\n\")\n",
    "\n",
    "# Create total_rooms by summing all room types\n",
    "df_model['total_rooms'] = df_model[available_cols].sum(axis=1)\n",
    "\n",
    "# ============================================================================\n",
    "# RE-RUN CORRELATION ANALYSIS WITH NEW FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "# Select numeric columns for correlation (excluding flags and derived columns)\n",
    "numeric_cols = ['price_eur', 'area_sqm', 'floor', 'total_rooms', 'distance_from_center']\n",
    "numeric_cols = [col for col in numeric_cols if col in df_model.columns]\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df_model[numeric_cols].corr()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"UPDATED CORRELATION MATRIX\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE UPDATED CORRELATION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=True,\n",
    "            fmt='.4f',\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "\n",
    "plt.title('Correlation Matrix - Including Total Rooms', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d5db3-b261-4f6a-b296-70d28a233b69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "632d5db3-b261-4f6a-b296-70d28a233b69",
    "outputId": "7e61d59c-6285-487f-bef1-118cf8d97411"
   },
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7545734-6921-4b1d-a8f2-e74f62807a47",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "<h2 style=\"color:#2c3e50;\">üõ†Ô∏è Phase 4: Data Preprocessing</h2>\n",
    "<h3 style=\"color:#2c3e50;\">One-Hot Encoding & Feature Preprocessing</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We preprocess the dataset by keeping numeric features as is and applying <b>one-hot encoding</b> to categorical columns like <code>furnishing_status</code>. \n",
    "Text columns such as <code>description</code> are dropped. This creates a clean, machine-learning-ready dataset with all categorical variables converted into numeric format.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ Numeric features: price, area, floor, rooms, elevators, parking, location, etc.</li>\n",
    "  <li>üîπ Categorical features: converted to binary columns using <code>OneHotEncoder</code> with <code>drop='first'</code> to avoid multicollinearity.</li>\n",
    "  <li>üîπ Result: a fully numeric DataFrame <code>df_encoded</code> suitable for ML pipelines.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a67c84-df82-484f-9c3e-448b552245b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58a67c84-df82-484f-9c3e-448b552245b2",
    "outputId": "19e7e56d-e00e-4db5-bd62-67156af4b317"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify categorical columns to encode\n",
    "categorical_features = [\n",
    "    'furnishing_status'\n",
    "]\n",
    "\n",
    "# Identify numerical features (keep as is)\n",
    "numerical_features = [\n",
    "    'price_eur',\n",
    "    'area_sqm',\n",
    "    'floor',\n",
    "    'bedrooms',\n",
    "    'bathrooms',\n",
    "    'balconies',\n",
    "    'living_rooms',\n",
    "    'has_elevator',\n",
    "    'has_parking_space',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'price_per_sqm',\n",
    "    'is_price_outlier',\n",
    "    'distance_from_center',\n",
    "    'total_rooms'\n",
    "]\n",
    "\n",
    "# Features to drop (text descriptions, coordinates already captured, etc.)\n",
    "features_to_drop = ['description']  # Drop long text columns\n",
    "\n",
    "# Method 1: Using sklearn ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(\n",
    "            sparse_output=False,\n",
    "            handle_unknown='ignore',\n",
    "            drop='first'  # Avoid multicollinearity\n",
    "        ), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(df_model)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = (\n",
    "    numerical_features +\n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Create new DataFrame with encoded features\n",
    "df_encoded = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Encoded shape:\", df_encoded.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0eb5d4-630a-4aca-967b-efc16ba557e2",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h2 style=\"color:#2c3e50;\">‚ö° Phase 5: Train/Test Split & Standard Scaling</h2>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "We prepare the dataset for machine learning by splitting it into <b>training</b> and <b>testing</b> sets and standardizing numeric features. \n",
    "Standard scaling ensures all numeric variables have a mean of 0 and a standard deviation of 1, improving model performance and convergence.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Features & Target:</b> <code>X</code> contains all predictors, <code>y</code> is <code>price_eur</code>.</li>\n",
    "  <li>üîπ <b>Numeric columns scaled:</b> area, floor, rooms, location, price_per_sqm, distance_from_center, total_rooms.</li>\n",
    "  <li>üîπ <b>Train/Test Split:</b> 80% train, 20% test (<code>random_state=42</code> for reproducibility).</li>\n",
    "  <li>üîπ <b>Scaling:</b> StandardScaler applied to numeric columns, with rounded output for clean inspection.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "After scaling, the training set numeric features have mean ~0 and std ~1, ensuring consistency across features for machine learning models.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abe72b-3402-4741-93d9-755a0ad7ae64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86abe72b-3402-4741-93d9-755a0ad7ae64",
    "outputId": "45b6e114-16e1-4c56-c123-394b725dd88a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Features & Target\n",
    "# -------------------------------\n",
    "feature_cols = ['area_sqm', 'floor', 'lat', 'lng','total_rooms']\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['price_eur']\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Numerical columns\n",
    "# -------------------------------\n",
    "num_cols = [\n",
    "    'area_sqm', 'floor', 'bedrooms', 'bathrooms',\n",
    "    'balconies', 'living_rooms', 'lat', 'lng',\n",
    "    'price_per_sqm', 'distance_from_center', 'total_rooms'\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Train / Test Split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Standard Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train[feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "X_test[feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Round values (for clean output)\n",
    "# -------------------------------\n",
    "X_train[feature_cols] = X_train[feature_cols].round(3)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. AFTER SCALING summary\n",
    "# -------------------------------\n",
    "print(\"\\nAFTER SCALING (TRAIN SET)\")\n",
    "print(X_train[feature_cols].describe().round(3).loc[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_uq2YTAGuZm",
   "metadata": {
    "id": "F_uq2YTAGuZm"
   },
   "source": [
    "<h2 style=\"color:#2c3e50;\">üß† Phase 6: Model Definition & Setup</h2> <p style=\"font-size:14px; line-height:1.5;\"> This phase defines a <b>set of regression models</b> to compare. The goal is to test different learning approaches‚Äîfrom simple linear models to advanced ensemble methods‚Äîand see which best captures the relationship between features and target price. </p> <ul style=\"font-size:14px;\"> <li>üîπ Includes <b>linear models</b> (Linear, Ridge, Lasso)</li> <li>üîπ Includes <b>distance-based</b> learning (KNN)</li> <li>üîπ Includes <b>tree-based ensembles</b> (Random Forest, Gradient Boosting)</li> <li>üîπ Optionally includes <b>XGBoost</b> if installed</li> </ul> <p style=\"font-size:14px;\"> Each model is tagged with a <b>scaled flag</b> indicating whether feature scaling is required: </p> <ul style=\"font-size:14px;\"> <li>üìè <b>Scaled = True</b> ‚Üí Linear, Ridge, Lasso, KNN</li> <li>üå≥ <b>Scaled = False</b> ‚Üí Tree-based models (scale-invariant)</li> </ul>\n",
    "<h3 style=\"color:#2c3e50;\">üèãÔ∏èTraining & Evaluation</h3> <p style=\"font-size:14px; line-height:1.5;\"> In this phase, every model is <b>trained, tested, and evaluated consistently</b> using the same dataset splits. This ensures a fair comparison across algorithms. </p> <ul style=\"font-size:14px;\"> <li>üîπ Train each model on <code>X_train</code> and <code>y_train</code></li> <li>üîπ Predict on unseen test data (<code>X_test</code>)</li> <li>üîπ Handle errors gracefully so one failure doesn‚Äôt stop the pipeline</li> </ul> <p style=\"font-size:14px;\"> For each model, the following <b>performance metrics</b> are computed: </p> <ul style=\"font-size:14px;\"> <li>üìâ <b>RMSE</b> ‚Äì Penalizes large pricing errors</li> <li>üìè <b>MAE</b> ‚Äì Average absolute error in euros</li> <li>üìä <b>R¬≤ Score</b> ‚Äì How much variance the model explains</li> <li>üîÅ <b>Cross-Validation R¬≤</b> ‚Äì Stability across 5 folds</li> </ul> <p style=\"font-size:14px;\"> All results are stored in a central <code>results</code> dictionary for later comparison. </p>\n",
    "<h3 style=\"color:#2c3e50;\">üìä Model Comparison & Ranking</h3> <p style=\"font-size:14px; line-height:1.5;\"> Once all models are evaluated, they are <b>ranked by R¬≤ score</b> to identify the strongest performer on the test set. </p> <ul style=\"font-size:14px;\"> <li>üîπ Models are sorted from best to worst</li> <li>üîπ Metrics are printed in a clean leaderboard format</li> <li>üîπ The top-performing model is selected as the <b>best model</b></li> </ul> <p style=\"font-size:14px;\"> This makes it easy to balance <b>accuracy, robustness, and complexity</b> when choosing the final model. </p>\n",
    "<h3 style=\"color:#2c3e50;\">üîçModel Interpretability</h3> <p style=\"font-size:14px; line-height:1.5;\"> After selecting the best model, the code explains <b>why</b> the model makes its predictions by analyzing feature impact. </p> <h3 style=\"color:#34495e;\">üå≥ Tree-Based Models</h3> <ul style=\"font-size:14px;\"> <li>üîπ Uses <code>feature_importances_</code></li> <li>üîπ Ranks features by contribution to splits</li> <li>üîπ Highlights what drives prices most strongly</li> </ul> <h3 style=\"color:#34495e;\">üìê Linear Models</h3> <ul style=\"font-size:14px;\"> <li>üîπ Examines regression coefficients</li> <li>üîπ Sorts features by absolute effect size</li> <li>üîπ Shows direction (+ / ‚àí) and strength of influence</li> </ul> <p style=\"font-size:14px;\"> This phase bridges the gap between <b>prediction and explanation</b>. </p>\n",
    "<h3 style=\"color:#2c3e50;\">üß™ Sample Predictions</h3> <p style=\"font-size:14px; line-height:1.5;\"> To ground the evaluation in real-world terms, the best model is used to generate <b>example predictions</b> on individual listings. </p> <ul style=\"font-size:14px;\"> <li>üîπ Compares predicted vs actual price</li> <li>üîπ Displays property characteristics (area, floor, rooms)</li> <li>üîπ Calculates absolute and percentage error</li> </ul> <p style=\"font-size:14px;\"> This makes the model‚Äôs performance <b>tangible and interpretable</b>, especially for non-technical stakeholders. </p>\n",
    "<h3 style=\"color:#2c3e50;\">‚úÖ Final Outcome</h3> <p style=\"font-size:14px; line-height:1.5;\"> This pipeline delivers a <b>complete, production-ready modeling workflow</b>: </p> <ul style=\"font-size:14px;\"> <li>‚úî Multiple algorithms tested fairly</li> <li>‚úî Robust evaluation with cross-validation</li> <li>‚úî Clear model selection logic</li> <li>‚úî Built-in explainability</li> <li>‚úî Real-world sanity checks</li> </ul> <p style=\"font-size:14px;\"> It‚Äôs an excellent foundation for a <b>real estate price prediction system</b> and aligns well with EU-style expectations for transparency and accountability in ML models. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f097a4-8327-4a83-a547-f38ab988844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Define models\n",
    "# -------------------------------\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
    "    xgb_available = False\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'scaled': True\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(alpha=1000),\n",
    "        'scaled': True\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(alpha=100),\n",
    "        'scaled': True\n",
    "    },\n",
    "    'KNN Regressor': {\n",
    "        'model': KNeighborsRegressor(n_neighbors=5, n_jobs=-1),\n",
    "        'scaled': True\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'scaled': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'scaled': False\n",
    "    }\n",
    "}\n",
    "# Add XGBoost if available\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = {\n",
    "        'model': xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'scaled': False\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Train and evaluate all models\n",
    "# -------------------------------\n",
    "results = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    model = config['model']\n",
    "    use_scaled = config['scaled']\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        X_train_use = X_train\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        cv_score = cross_val_score(model, X_train_use, y_train, cv=5, \n",
    "                                    scoring='r2').mean()\n",
    "        \n",
    "        results[name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R¬≤ Score': r2,\n",
    "            'CV R¬≤ Score': cv_score,\n",
    "            'model': model,\n",
    "            'use_scaled': use_scaled\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  RMSE:          ‚Ç¨{rmse:,.2f}\")\n",
    "        print(f\"  MAE:           ‚Ç¨{mae:,.2f}\")\n",
    "        print(f\"  R¬≤ Score:      {r2:.4f}\")\n",
    "        print(f\"  CV R¬≤ Score:   {cv_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: ERROR - {str(e)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Summary and best model\n",
    "# -------------------------------\n",
    "\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY - RANKED BY R¬≤ SCORE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['R¬≤ Score'], reverse=True)\n",
    "    \n",
    "    for rank, (name, metrics) in enumerate(sorted_results, 1):\n",
    "        print(f\"\\n{rank}. {name}\")\n",
    "        print(f\"   R¬≤ Score: {metrics['R¬≤ Score']:.4f}\")\n",
    "        print(f\"   RMSE:     ‚Ç¨{metrics['RMSE']:,.2f}\")\n",
    "        print(f\"   MAE:      ‚Ç¨{metrics['MAE']:,.2f}\")\n",
    "        print(f\"   CV R¬≤:    {metrics['CV R¬≤ Score']:.4f}\")\n",
    "    \n",
    "    best_model_name = sorted_results[0][0]\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   R¬≤ Score: {results[best_model_name]['R¬≤ Score']:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
    "        best_model = results[best_model_name]['model']\n",
    "        importances = best_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        print(f\"\\nFeature Importance ({best_model_name}):\")\n",
    "        for i, idx in enumerate(indices, 1):\n",
    "            print(f\"  {i}. {feature_cols[idx]:30s} {importances[idx]:.4f}\")\n",
    "    \n",
    "    # Coefficient analysis for linear models\n",
    "    elif best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression']:\n",
    "        best_model = results[best_model_name]['model']\n",
    "        coefs = best_model.coef_\n",
    "        \n",
    "        print(f\"\\nRegression Coefficients ({best_model_name}):\")\n",
    "        feature_importance = list(zip(feature_cols, coefs))\n",
    "        feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        for feature, coefficient in feature_importance:\n",
    "            print(f\"{feature:<30} {coefficient:>10.2f}\")\n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx in range(min(5, len(X_test))):\n",
    "        sample = X_test.iloc[idx:idx+1]\n",
    "        actual = y_test.iloc[idx]\n",
    "        \n",
    "        if results[best_model_name]['model'] is not None:\n",
    "            if results[best_model_name]['use_scaled']:\n",
    "                sample_scaled = scaler.transform(sample)\n",
    "                prediction = results[best_model_name]['model'].predict(sample_scaled)[0]\n",
    "            else:\n",
    "                prediction = results[best_model_name]['model'].predict(sample)[0]\n",
    "            \n",
    "            error_pct = abs(prediction - actual) / actual * 100\n",
    "            print(f\"\\nSample {idx+1}:\")\n",
    "            print(f\"  Area: {sample['area_sqm'].values[0]:.2f} m¬≤ | Floor: {sample['floor'].values[0]:.0f} | Rooms: {sample['total_rooms'].values[0]:.0f}\")\n",
    "            print(f\"  Predicted: ‚Ç¨{prediction:,.2f}\")\n",
    "            print(f\"  Actual:    ‚Ç¨{actual:,.2f}\")\n",
    "            print(f\"  Error:     ‚Ç¨{abs(prediction - actual):,.2f} ({error_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7002b52-ee62-4e27-8138-2fa2f76dd550",
   "metadata": {
    "id": "c7002b52-ee62-4e27-8138-2fa2f76dd550"
   },
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üìä Tirana House Prices Final Dashboard</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This dashboard provides a comprehensive visual analysis of apartment listings in Tirana, using a <b>cohesive color palette</b> to highlight key patterns and distributions.\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Box Plot Comparison:</b> Shows the <b>price per m¬≤ distribution</b> before and after outlier removal.</li>\n",
    "  <li>üîπ <b>Price Distribution:</b> Histogram of apartment prices capped at the 99th percentile for clarity.</li>\n",
    "  <li>üîπ <b>Area Distribution:</b> Histogram of apartment sizes (sqm) with 99th percentile cap.</li>\n",
    "  <li>üîπ <b>Distance from Center:</b> Density plot highlighting how far apartments are from the city center.</li>\n",
    "  <li>üîπ <b>Average Price by Size:</b> Line chart showing the trend of average price as <b>apartment area increases</b>.</li>\n",
    "  <li>üîπ <b>Correlation Matrix:</b> Heatmap of key numeric features including <b>total_rooms</b> and <b>price_per_sqm</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "The dashboard uses <b>professional color coding</b> for clear distinction between metrics, facilitating rapid insight into pricing patterns, room characteristics, and spatial distribution for real estate analysis in Tirana.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e1986-b805-4012-9903-02f384e3977c",
   "metadata": {
    "id": "371e1986-b805-4012-9903-02f384e3977c"
   },
   "outputs": [],
   "source": [
    "# Set a cohesive color palette\n",
    "# Using a professional palette with blues, teals, and complementary warm colors\n",
    "COLOR_PALETTE = {\n",
    "    'primary': '#A8DADC',      # Soft cyan\n",
    "    'secondary': '#E5C3D1',    # Pastel pink\n",
    "    'accent1': '#F4C2A0',      # Peachy pastel\n",
    "    'accent2': '#D4A5A5',      # Dusty rose\n",
    "    'neutral': '#C9C9C9',      # Soft gray\n",
    "    'teal': '#9EC5C8',         # Pastel teal\n",
    "    'light_blue': '#B4D4E1',   # Powder blue\n",
    "    'warm': '#E8D5B7'          # Warm cream\n",
    "}\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette([COLOR_PALETTE['primary'], COLOR_PALETTE['secondary'],\n",
    "                 COLOR_PALETTE['accent1'], COLOR_PALETTE['teal']])\n",
    "\n",
    "# Create the main dashboard figure\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3,\n",
    "                      left=0.05, right=0.98, top=0.95, bottom=0.05)\n",
    "\n",
    "# Add main title\n",
    "fig.suptitle('Tirana House Prices EDA Dashboard',\n",
    "             fontsize=20, fontweight='bold', y=1)\n",
    "\n",
    "# 1. Box Plot Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "box_data = [df_clean['price_per_sqm'], df_analysis['price_per_sqm']]\n",
    "bp = ax1.boxplot(box_data, labels=['Before', 'After'], patch_artist=True,\n",
    "                 widths=0.6)\n",
    "bp['boxes'][0].set_facecolor(COLOR_PALETTE['light_blue'])\n",
    "bp['boxes'][1].set_facecolor(COLOR_PALETTE['teal'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "    box.set_linewidth(1.5)\n",
    "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(bp[element], linewidth=1.5)\n",
    "ax1.set_ylabel(\"Price per m¬≤ (‚Ç¨)\", fontsize=10)\n",
    "ax1.set_title(\"Distribution Comparison\", fontsize=12, fontweight='bold', pad=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Price Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "df_plot = df_model[df_model[\"price_eur\"] < df_model[\"price_eur\"].quantile(0.99)]\n",
    "sns.histplot(data=df_plot, x=\"price_eur\", bins=50, kde=True,\n",
    "             color=COLOR_PALETTE['primary'], alpha=0.7, ax=ax2)\n",
    "ax2.set_title(\"Price Distribution (EUR)\", fontsize=12, fontweight='bold', pad=10)\n",
    "ax2.set_xlabel(\"Price (EUR)\", fontsize=10)\n",
    "ax2.set_ylabel(\"Count\", fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Area Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "df_plot = df_model[df_model[\"area_sqm\"] < df_model[\"area_sqm\"].quantile(0.99)]\n",
    "sns.histplot(data=df_plot, x=\"area_sqm\", bins=50, kde=True,\n",
    "             color=COLOR_PALETTE['accent1'], alpha=0.7, ax=ax3)\n",
    "ax3.set_title(\"Area Distribution (sqm)\", fontsize=12, fontweight='bold', pad=10)\n",
    "ax3.set_xlabel(\"Area (sqm)\", fontsize=10)\n",
    "ax3.set_ylabel(\"Count\", fontsize=10)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Distance from Center\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "df_plot = df_model[df_model[\"distance_from_center\"] < df_model[\"distance_from_center\"].quantile(0.99)]\n",
    "sns.histplot(data=df_plot, x=\"distance_from_center\", bins=50, kde=True,\n",
    "             stat=\"density\", color=COLOR_PALETTE['secondary'], alpha=0.7, ax=ax4)\n",
    "ax4.set_title(\"Distance from Center Density\", fontsize=12, fontweight='bold', pad=10)\n",
    "ax4.set_xlabel(\"Distance (km)\", fontsize=10)\n",
    "ax4.set_ylabel(\"Density\", fontsize=10)\n",
    "ax4.set_xlim(0, 20)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. Average Price by Area Group\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "df_model[\"area_group\"] = pd.cut(\n",
    "    df_model[\"area_sqm\"],\n",
    "    bins=[0, 40, 60, 80, 100, 150, 300],\n",
    "    labels=[\"0-40\", \"40-60\", \"60-80\", \"80-100\", \"100-150\", \"150+\"]\n",
    ")\n",
    "area_mean = df_model.groupby(\"area_group\")[\"price_eur\"].mean().reset_index()\n",
    "ax5.plot(area_mean[\"area_group\"], area_mean[\"price_eur\"],\n",
    "         marker=\"o\", linewidth=3, markersize=10,\n",
    "         color=COLOR_PALETTE['primary'], markerfacecolor=COLOR_PALETTE['accent1'],\n",
    "         markeredgewidth=2, markeredgecolor=COLOR_PALETTE['primary'])\n",
    "ax5.set_title(\"Average Price by Apartment Size\", fontsize=12, fontweight='bold', pad=10)\n",
    "ax5.set_xlabel(\"Area Group (sqm)\", fontsize=10)\n",
    "ax5.set_ylabel(\"Average Price (EUR)\", fontsize=10)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'‚Ç¨{x/1000:.0f}K'))\n",
    "\n",
    "\n",
    "# 10. Correlation Matrix\n",
    "ax10 = fig.add_subplot(gs[1:, 1:])\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f',\n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={'label': 'Correlation', 'shrink': 0.8},\n",
    "            ax=ax10, vmin=-1, vmax=1, annot_kws={'size': 10})\n",
    "ax10.set_title('Correlation Matrix - Key Features', fontsize=12, fontweight='bold', pad=10)\n",
    "ax10.set_xticklabels(ax10.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "ax10.set_yticklabels(ax10.get_yticklabels(), rotation=0, fontsize=9)\n",
    "\n",
    "plt.savefig('Tirana_House_Prices_EDA_Dashboard.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228f8c6-f120-436b-b127-abfade0f164d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid #ddd;\">\n",
    "\n",
    "<h3 style=\"color:#2c3e50;\">üó∫Ô∏è Tirana Apartment Prices Interactive Map</h3>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "This interactive map visualizes apartment listings in Tirana using <b>latitude and longitude coordinates</b>. The map includes two layers:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:15px;\">\n",
    "  <li>üîπ <b>Clustered Markers:</b> Groups nearby listings for a cleaner overview. Click a cluster to zoom in and explore individual apartments.</li>\n",
    "  <li>üîπ <b>Individual Markers:</b> Each apartment is displayed separately. The <b>size</b> of the marker is proportional to <b>price per m¬≤</b>.</li>\n",
    "  <li>üîπ <b>Color:</b> Represents the price per m¬≤:\n",
    "    <ul>\n",
    "      <li>üü¢ Green: Affordable (&lt; ‚Ç¨1,200)</li>\n",
    "      <li>üü† Orange: Mid-range (‚Ç¨1,200‚Äì‚Ç¨2,300)</li>\n",
    "      <li>üî¥ Red: Expensive (&gt; ‚Ç¨2,300)</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>üîπ <b>Popup:</b> Shows <b>total price</b>, <b>area</b>, <b>price per m¬≤</b>, and optionally <b>bedrooms</b>.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:15px; line-height:1.6;\">\n",
    "Use the layer control (top-right) to toggle between <b>Clustered</b> and <b>Individual</b> markers. The map is centered on <b>Tirana</b> and allows for intuitive exploration of <b>spatial price trends</b> across the city. It is saved as an HTML file for easy sharing and interactive use.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258f647-00ec-4b07-b0b2-4ce1968372be",
   "metadata": {
    "id": "c258f647-00ec-4b07-b0b2-4ce1968372be"
   },
   "outputs": [],
   "source": [
    "#!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad650d3-3c72-417c-b40e-b36370b76413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import Map, FeatureGroup, CircleMarker, LayerControl\n",
    "from folium.plugins import MarkerCluster\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "# --- Base map ---\n",
    "tirana_center = [41.3275, 19.8189]\n",
    "m = Map(location=tirana_center, zoom_start=12, tiles=\"cartodb positron\")\n",
    "\n",
    "# --- Layers ---\n",
    "cluster_layer = FeatureGroup(name=\"üü¢ Clustered Markers\", show=True)\n",
    "individual_layer = FeatureGroup(name=\"üîµ Individual Markers\", show=False)\n",
    "\n",
    "# --- Example adding markers to layers ---\n",
    "marker_cluster = MarkerCluster().add_to(cluster_layer)\n",
    "for _, row in df_map.iterrows():\n",
    "    CircleMarker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        radius=5,\n",
    "        color=price_to_color(row['price_per_sqm']),\n",
    "        fill=True,\n",
    "        fillColor=price_to_color(row['price_per_sqm']),\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(styled_popup(row, show_bedrooms=True), max_width=250)\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "for _, row in df_map.iterrows():\n",
    "    CircleMarker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        radius=price_to_radius(row['price_per_sqm']),\n",
    "        color=price_to_color(row['price_per_sqm']),\n",
    "        fill=True,\n",
    "        fillColor=price_to_color(row['price_per_sqm']),\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(styled_popup(row, show_bedrooms=False), max_width=250)\n",
    "    ).add_to(individual_layer)\n",
    "\n",
    "cluster_layer.add_to(m)\n",
    "individual_layer.add_to(m)\n",
    "\n",
    "# --- Styled LayerControl ---\n",
    "LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "# Inject CSS for LayerControl styling\n",
    "m.get_root().html.add_child(folium.Element(\"\"\"\n",
    "<style>\n",
    ".leaflet-control-layers {\n",
    "    font-family: 'Arial', sans-serif;\n",
    "    font-size: 14px;\n",
    "    background: #f7f7f7;\n",
    "    border-radius: 6px;\n",
    "    padding: 10px;\n",
    "    box-shadow: 0 2px 6px rgba(0,0,0,0.3);\n",
    "}\n",
    ".leaflet-control-layers label {\n",
    "    font-weight: bold;\n",
    "    margin-bottom: 4px;\n",
    "}\n",
    ".leaflet-control-layers input[type=\"checkbox\"] {\n",
    "    transform: scale(1.2);\n",
    "    margin-right: 6px;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "# --- Save & display ---\n",
    "m.save(\"cmimet_tirane_styled_layers_toggle.html\")\n",
    "display(IFrame(\"cmimet_tirane_styled_layers_toggle.html\", width=1000, height=1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b82db-3e36-4ed7-a088-c929a39dce59",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2c3e50;\">üîë Key Insight: Apartment Price Prediction Project</h2>\n",
    "<p style=\"font-size:14px; line-height:1.6;\">\n",
    "The project shows that predicting apartment prices in Tirana is <b>feasible but complex</b>. Key takeaways include:\n",
    "</p>\n",
    "<ul style=\"font-size:14px; line-height:1.5;\">\n",
    "    <li>üìä <b>Data Cleaning & Feature Engineering:</b> Extracted numerical features like area, bedrooms, bathrooms, total rooms, and distance from center; encoded categorical variables such as furnishing status to prepare the dataset for ML.</li>\n",
    "    <li>‚ö° <b>Baseline Modeling:</b> Linear Regression establishes a <b>performance benchmark</b> and reveals general relationships between features and price, but struggles with outliers and non-linear patterns.</li>\n",
    "    <li>üå≥ <b>Advanced Models:</b> Ensemble methods like Random Forest or Ridge Regression improve predictions by capturing complex interactions and reducing sensitivity to extreme values.</li>\n",
    "    <li>üîß <b>Scaling & Preprocessing:</b> Standard scaling of numerical features ensures stability and comparability across all features.</li>\n",
    "    <li>üìç <b>EDA & Insights:</b> Analysis of price per m¬≤, location, and total rooms provides actionable insights for investors or buyers.</li>\n",
    "    <li>üí° <b>Takeaway:</b> Combining <b>feature-rich datasets</b> with <b>appropriate models</b> produces reliable predictions, guides investment decisions, and identifies high-value areas in Tirana.</li>\n",
    "</ul>\n",
    "<p style=\"font-size:14px; line-height:1.6; color:#34495e;\">\n",
    "<b>Overall:</b> Thoughtful feature engineering and careful model selection are key to predicting apartment prices accurately.\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
